{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd \n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "def preds_to_df(preds_path):\n",
    "\n",
    "    coco_train = COCO('../coco/annotations/instances_train2014.json')\n",
    "    coco_val = COCO('../coco/annotations/instances_val2014.json')\n",
    "\n",
    "    with open(preds_path) as fp:\n",
    "        model_preds = json.load(fp)\n",
    "\n",
    "    preproc = []\n",
    "\n",
    "    for i in model_preds:\n",
    "        img_id = str(i['img_id'])\n",
    "        if 'val' in img_id:\n",
    "            loaded_img = coco_val.loadImgs([int(img_id[-6:])])\n",
    "        else:\n",
    "            loaded_img = coco_train.loadImgs([int(img_id[-6:])])\n",
    "        img_url = loaded_img[0]['coco_url']\n",
    "\n",
    "        label = i['label']\n",
    "        label_list = list(label.keys())\n",
    "        i['label'] = ', '.join(label_list)\n",
    "        i['img_url'] = img_url\n",
    "        preproc.append(i)\n",
    "\n",
    "    df = pd.json_normalize(preproc)\n",
    "\n",
    "    return df\n",
    "\n",
    "def validation_per_question(df):\n",
    "    validation_questions = {}\n",
    "    unique_questions = df['question_type'].unique()\n",
    "    grouped = df.groupby(df.question_type)\n",
    "    for ques in unique_questions:\n",
    "        df1 = grouped.get_group(ques)\n",
    "        val_score = df1['score'].sum() / len(df1)\n",
    "        validation_questions[ques] = val_score\n",
    "    return validation_questions\n",
    "    \n",
    "def validation_per_answer(df):\n",
    "    validation_answers = {}\n",
    "    unique_answers= df['answer_type'].unique()\n",
    "    grouped = df.groupby(df.answer_type)\n",
    "    for ans in unique_answers:\n",
    "        df1 = grouped.get_group(ans)\n",
    "        val_score = df1['score'].sum() / len(df1)\n",
    "        validation_answers[ans] = val_score\n",
    "    return validation_answers\n",
    "\n",
    "\n",
    "def compare_dfs(full_df, sampled_df):\n",
    "    # segment all the preds that full model got wrong -- 0.0\n",
    "    # segment all the preds sampled model got right -- > 0.0 \n",
    "    # look at overlap between subsets and vice versa\n",
    "\n",
    "    full_df_wrong_preds = full_df[full_df['score'] == 0.0]\n",
    "    sampled_df_wrong_preds = sampled_df[sampled_df['score'] == 0.0]\n",
    "\n",
    "    full_df_right_preds = full_df[full_df['score'] > 0.0]\n",
    "    sampled_df_right_preds = sampled_df[sampled_df['score'] > 0.0]\n",
    "\n",
    "    # for the overlap, question types, target types, and for each quest type and target type, plot a few\n",
    "    # independent of overlap, look at what each model is getting wrong -- question types, target types, and for each quest type and target type, plot a few\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.45s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=5.25s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "normal_near_mode_var_path = '/home/jaspreet/vl-pretraining/snap/vqa/lxr111_multilabel_normal_near_mode_var0.1_0.4_results/minival_predict.json'\n",
    "#global_random_path = ''\n",
    "beta_pvals_var_path = '/home/jaspreet/vl-pretraining/snap/vqa/lxr111_multilabel_beta_pvals_var_21_results/minival_predict.json'\n",
    "full_model_path = '/home/jaspreet/vl-pretraining/snap/vqa/lxr111_multilabel_full_run_results/minival_predict.json'\n",
    "df = preds_to_df(normal_near_mode_var_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  what color is the\n",
      "Score:  0.7312953995157384\n",
      "Question:  what type of\n",
      "Score:  0.5243353783231084\n",
      "Question:  why\n",
      "Score:  0.15159574468085107\n",
      "Question:  what kind of\n",
      "Score:  0.5130116959064327\n",
      "Question:  what is the\n",
      "Score:  0.42981684981684987\n",
      "Question:  are these\n",
      "Score:  0.7264864864864865\n",
      "Question:  none of the above\n",
      "Score:  0.5401886792452829\n",
      "Question:  what is on the\n",
      "Score:  0.40355731225296443\n",
      "Question:  is this\n",
      "Score:  0.7249492900608518\n",
      "Question:  how many\n",
      "Score:  0.4489361702127659\n",
      "Question:  does this\n",
      "Score:  0.7291338582677165\n",
      "Question:  does the\n",
      "Score:  0.7294594594594594\n",
      "Question:  is the person\n",
      "Score:  0.7413461538461538\n",
      "Question:  are\n",
      "Score:  0.6915492957746479\n",
      "Question:  how many people are\n",
      "Score:  0.5265560165975104\n",
      "Question:  is the man\n",
      "Score:  0.7443661971830986\n",
      "Question:  do\n",
      "Score:  0.7427710843373495\n",
      "Question:  is it\n",
      "Score:  0.8524137931034482\n",
      "Question:  are there\n",
      "Score:  0.762883435582822\n",
      "Question:  what\n",
      "Score:  0.4233468972533062\n",
      "Question:  is the\n",
      "Score:  0.7367525522605737\n",
      "Question:  is there\n",
      "Score:  0.731358024691358\n",
      "Question:  are the\n",
      "Score:  0.7111111111111111\n",
      "Question:  is that a\n",
      "Score:  0.6895833333333333\n",
      "Question:  who is\n",
      "Score:  0.3421875\n",
      "Question:  are there any\n",
      "Score:  0.715483870967742\n",
      "Question:  what is in the\n",
      "Score:  0.4436464088397789\n",
      "Question:  was\n",
      "Score:  0.8178217821782178\n",
      "Question:  what is\n",
      "Score:  0.3790155440414508\n",
      "Question:  is there a\n",
      "Score:  0.6885714285714286\n",
      "Question:  what is the person\n",
      "Score:  0.650574712643678\n",
      "Question:  which\n",
      "Score:  0.43838028169014087\n",
      "Question:  where is the\n",
      "Score:  0.32887931034482754\n",
      "Question:  what does the\n",
      "Score:  0.24210526315789477\n",
      "Question:  is this a\n",
      "Score:  0.7441441441441441\n",
      "Question:  is\n",
      "Score:  0.7221153846153846\n",
      "Question:  how\n",
      "Score:  0.24385382059800667\n",
      "Question:  what color are the\n",
      "Score:  0.7093198992443326\n",
      "Question:  what is the man\n",
      "Score:  0.5947194719471947\n",
      "Question:  what room is\n",
      "Score:  0.9106796116504853\n",
      "Question:  is he\n",
      "Score:  0.7852941176470588\n",
      "Question:  what time\n",
      "Score:  0.26045454545454544\n",
      "Question:  can you\n",
      "Score:  0.6185185185185186\n",
      "Question:  what are the\n",
      "Score:  0.4648241206030151\n",
      "Question:  has\n",
      "Score:  0.6775510204081632\n",
      "Question:  what is the name\n",
      "Score:  0.09195402298850573\n",
      "Question:  what is this\n",
      "Score:  0.552582159624413\n",
      "Question:  what number is\n",
      "Score:  0.055696202531645575\n",
      "Question:  what are\n",
      "Score:  0.5265624999999999\n",
      "Question:  are they\n",
      "Score:  0.7248520710059172\n",
      "Question:  could\n",
      "Score:  0.7342465753424656\n",
      "Question:  is this person\n",
      "Score:  0.7448979591836735\n",
      "Question:  what color\n",
      "Score:  0.664039408866995\n",
      "Question:  what animal is\n",
      "Score:  0.5967391304347827\n",
      "Question:  what brand\n",
      "Score:  0.4186991869918699\n",
      "Question:  what sport is\n",
      "Score:  0.9230769230769231\n",
      "Question:  what color is\n",
      "Score:  0.8072847682119205\n",
      "Question:  is this an\n",
      "Score:  0.6125\n",
      "Question:  is the woman\n",
      "Score:  0.7444444444444445\n",
      "Question:  what is the woman\n",
      "Score:  0.6438095238095237\n",
      "Question:  what is the color of the\n",
      "Score:  0.7174757281553399\n",
      "Question:  how many people are in\n",
      "Score:  0.49687499999999996\n",
      "Question:  why is the\n",
      "Score:  0.154\n",
      "Question:  do you\n",
      "Score:  0.7373626373626373\n",
      "Question:  where are the\n",
      "Score:  0.29520547945205483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'other': 0.5070316186880603,\n",
       " 'yes/no': 0.7418102130261952,\n",
       " 'number': 0.3946382242721245}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_per_question(df)\n",
    "validation_per_answer(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lxmert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
