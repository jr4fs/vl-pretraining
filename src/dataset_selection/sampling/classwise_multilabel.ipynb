{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9548/1250726028.py:19: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import math \n",
    "import collections\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import base64\n",
    "import itertools\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "sns.set(style='whitegrid', font_scale=1.6, font='Georgia', context='paper')\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "import pickle \n",
    "from sklearn.neighbors import KernelDensity\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from random import sample\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_confidence_multilabel(df, model, training_budget, dataset='animals'):\n",
    "    #df = df.sample(frac=0.5, replace=False, random_state=1)\n",
    "    targets = df['Target'].tolist()\n",
    "    targets = [i[0] for i in targets]\n",
    "    df['Target'] = targets\n",
    "    orig_df = df.copy()\n",
    "    all_ids = df['question_id'].to_list()\n",
    "\n",
    "    # sampled_question_ids = []\n",
    "    # sampled_variabilities = []\n",
    "    # sampled_confidence = []\n",
    "    # sampled_correctness = []\n",
    "    # sampled_targets = []\n",
    "\n",
    "    #unique_targets = df['Target'].unique()\n",
    "    targets_list = np.array(df['Target'].tolist())\n",
    "    question_ids = np.array(df['question_id'].to_list())\n",
    "    confidence = np.array(df['confidence'].tolist())\n",
    "\n",
    "\n",
    "    targets_multilabel =[] # unique targets in vqa dataset \n",
    "    for i in targets_list:\n",
    "        target_list = [x.strip() for x in i.split(',')]\n",
    "        targets_multilabel.extend(target_list)\n",
    "    unique_targets = set(targets_multilabel)\n",
    "\n",
    "    class_buckets = {}\n",
    "    # class_lengths = {}\n",
    "    for label in unique_targets:\n",
    "        class_buckets[label] = {}\n",
    "        # class_lengths[label] = 0\n",
    "\n",
    "    for idx, target_name in tqdm(enumerate(targets_list)):\n",
    "        target_list = [x.strip() for x in target_name.split(',')]\n",
    "        for target_label in target_list:\n",
    "            class_buckets[target_label][question_ids[idx]] = confidence[idx]\n",
    "    print(len(class_buckets))\n",
    "\n",
    "    \n",
    "    # for label in tqdm(unique_targets):\n",
    "    #     for idx, target_name in enumerate(targets_list):\n",
    "    #         if label in target_name:\n",
    "    #             class_buckets[label][question_ids[idx]] = confidence[idx] # map the target to a question id and its associated confidence\n",
    "# class_buckets now contains all the labels mapped to a dictionary that contains question_ids:confidence where question_id's target has that label\n",
    "\n",
    "    # for label in unique_targets:\n",
    "    #     class_lengths[label] = len(class_buckets[label])\n",
    "    \n",
    "    #class_buckets_sorted_length = OrderedDict(sorted(class_buckets.items(), key = lambda x : len(x[1])))\n",
    "    class_buckets_sorted_length = class_buckets\n",
    "\n",
    "    total_samples = round((training_budget*0.01) * len(all_ids))\n",
    "    final_sample = []\n",
    "\n",
    "    #while len(final_sample) < total_samples:\n",
    "    class_buckets_sorted_confidence = {}\n",
    "    for label in class_buckets_sorted_length:\n",
    "        class_buckets_sorted_confidence[label] = []\n",
    "\n",
    "    for label in tqdm(class_buckets_sorted_length):\n",
    "        label_confidence_sorted = dict(sorted(class_buckets_sorted_length[label].items(), key=lambda x:x[1], reverse=True)) # sort each class by confidence\n",
    "        label_ids = list(label_confidence_sorted.keys())\n",
    "        label_ids_unique = list(set(label_ids).difference(set(final_sample)))\n",
    "\n",
    "        #class_buckets_sorted_confidence[label].extend(list(label_confidence_sorted.keys()))\n",
    "        class_buckets_sorted_confidence[label].extend(label_ids_unique)\n",
    "        \n",
    "        num_samples = round(len(class_buckets_sorted_confidence[label]) * (training_budget*0.01))\n",
    "        if len(class_buckets_sorted_confidence[label]) == 0:\n",
    "            num_samples = 0\n",
    "        else:\n",
    "            if num_samples == 0:\n",
    "                num_samples = 1\n",
    "\n",
    "        added_samples = 0\n",
    "        label_idx = 0\n",
    "        while added_samples <= num_samples and label_idx < len(class_buckets_sorted_confidence[label]):\n",
    "            added_samples +=1\n",
    "            if class_buckets_sorted_confidence[label][label_idx] not in final_sample:\n",
    "                #added_samples +=1\n",
    "                final_sample.append(class_buckets_sorted_confidence[label][label_idx]) \n",
    "                \n",
    "            label_idx+=1\n",
    "\n",
    "\n",
    "    samples_left = total_samples - len(final_sample) \n",
    "    new_budget = (samples_left / total_samples) * 100\n",
    "    class_buckets_sorted_confidence = {}\n",
    "    for label in class_buckets_sorted_length:\n",
    "        class_buckets_sorted_confidence[label] = []\n",
    "    for label in tqdm(class_buckets_sorted_length):\n",
    "        label_confidence_sorted = dict(sorted(class_buckets_sorted_length[label].items(), key=lambda x:x[1], reverse=True)) # sort each class by confidence\n",
    "        label_ids = list(label_confidence_sorted.keys())\n",
    "        label_ids_unique = list(set(label_ids).difference(set(final_sample)))\n",
    "\n",
    "        #class_buckets_sorted_confidence[label].extend(list(label_confidence_sorted.keys()))\n",
    "        class_buckets_sorted_confidence[label].extend(label_ids_unique)\n",
    "        \n",
    "        num_samples = round(len(class_buckets_sorted_confidence[label]) * (new_budget*0.01))\n",
    "        if len(class_buckets_sorted_confidence[label]) == 0:\n",
    "            num_samples = 0\n",
    "        else:\n",
    "            if num_samples == 0:\n",
    "                num_samples = 1\n",
    "\n",
    "            added_samples = 0\n",
    "            label_idx = 0\n",
    "            while added_samples <= num_samples and label_idx < len(class_buckets_sorted_confidence[label]):\n",
    "                added_samples +=1\n",
    "                if class_buckets_sorted_confidence[label][label_idx] not in final_sample:\n",
    "                    #added_samples +=1\n",
    "                    final_sample.append(class_buckets_sorted_confidence[label][label_idx]) \n",
    "                    \n",
    "                label_idx+=1\n",
    "\n",
    "\n",
    "\n",
    "    #total_samples = round((training_budget*0.01) * len(all_ids))\n",
    "    print(\"Total samples: \", total_samples)\n",
    "    print(\"sample size: \", len(set(final_sample)))\n",
    "    #final_sample = sample(final_sample, total_samples)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # for label in tqdm(class_buckets_sorted_confidence):\n",
    "    #     num_samples = round(len(class_buckets_sorted_confidence[label]) * (training_budget*0.01))\n",
    "    #     if num_samples == 0:\n",
    "    #         num_samples = 1\n",
    "    #     added_samples = 0\n",
    "    #     label_idx = 0\n",
    "    #     while added_samples <= num_samples and label_idx < len(class_buckets_sorted_confidence[label]):\n",
    "    #         if class_buckets_sorted_confidence[label][label_idx] not in final_sample:\n",
    "    #             final_sample.append(class_buckets_sorted_confidence[label][label_idx])\n",
    "    #             added_samples +=1\n",
    "    #         label_idx+=1\n",
    "\n",
    "    save_path = '../../../src/dataset_selection/sampling/samples/'+model+'/'+dataset+'/max_confidence/seed_'+str(965)+'/budget_'+str(training_budget)+'random_order.pkl'\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(list(set(final_sample)), f)\n",
    "\n",
    "    unique_targets_sample = orig_df[orig_df['question_id'].isin(final_sample)]\n",
    "    sampled_targets_unique =[] # unique targets in vqa dataset \n",
    "    targets_list_sampled = np.array(unique_targets_sample['Target'].tolist())\n",
    "    for i in targets_list_sampled:\n",
    "        target_list = [x.strip() for x in i.split(',')]\n",
    "        sampled_targets_unique.extend(target_list)\n",
    "    unique_targets_sampled = set(sampled_targets_unique)\n",
    "\n",
    "    # sampled_targets_unique = [] # unique targets in sampled data\n",
    "    # sampled_targets = set(unique_targets_sample['Target'].unique())\n",
    "    # for sample in sampled_targets:\n",
    "    #     target_list = [x.strip() for x in sample.split(',')]\n",
    "    #     sampled_targets_unique.extend(target_list)\n",
    "    # sampled_targets_unique = set(sampled_targets_unique)\n",
    "    targets_excluded = unique_targets - unique_targets_sampled\n",
    "    print(\"TARGETS excluded: \", len(targets_excluded))\n",
    "\n",
    "    print(\"unique targets max confidence per class: \", len(set(unique_targets_sampled)))\n",
    "    print('samples - max confidence per class: ', len(set(final_sample)))\n",
    "    print('all_samples - max confidence per class: ', len(all_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "619390it [00:03, 182799.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 848/3128 [08:34<23:53,  1.59it/s]   "
     ]
    }
   ],
   "source": [
    "\n",
    "sampling_dataset = 'multilabel_full'\n",
    "sampling_model = 'LXR111'\n",
    "\n",
    "\n",
    "base_path = '../../../snap/vqa/lxr111_multilabel_full_run_3/'\n",
    "df = pd.read_pickle(base_path+\"datamap_metrics.pkl\")\n",
    "max_confidence_multilabel(df, sampling_model, 30, dataset=sampling_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min Confidence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_confidence_multilabel(df, model, training_budget, dataset='animals'):\n",
    "    #df = df.sample(frac=0.5, replace=False, random_state=1)\n",
    "    targets = df['Target'].tolist()\n",
    "    targets = [i[0] for i in targets]\n",
    "    df['Target'] = targets\n",
    "    orig_df = df.copy()\n",
    "    all_ids = df['question_id'].to_list()\n",
    "\n",
    "    # sampled_question_ids = []\n",
    "    # sampled_variabilities = []\n",
    "    # sampled_confidence = []\n",
    "    # sampled_correctness = []\n",
    "    # sampled_targets = []\n",
    "\n",
    "    #unique_targets = df['Target'].unique()\n",
    "    targets_list = np.array(df['Target'].tolist())\n",
    "    question_ids = np.array(df['question_id'].to_list())\n",
    "    confidence = np.array(df['confidence'].tolist())\n",
    "\n",
    "\n",
    "    targets_multilabel =[] # unique targets in vqa dataset \n",
    "    for i in targets_list:\n",
    "        target_list = [x.strip() for x in i.split(',')]\n",
    "        targets_multilabel.extend(target_list)\n",
    "    unique_targets = set(targets_multilabel)\n",
    "\n",
    "    class_buckets = {}\n",
    "    # class_lengths = {}\n",
    "    for label in unique_targets:\n",
    "        class_buckets[label] = {}\n",
    "        # class_lengths[label] = 0\n",
    "\n",
    "    for idx, target_name in tqdm(enumerate(targets_list)):\n",
    "        target_list = [x.strip() for x in target_name.split(',')]\n",
    "        for target_label in target_list:\n",
    "            class_buckets[target_label][question_ids[idx]] = confidence[idx]\n",
    "    print(len(class_buckets))\n",
    "\n",
    "    \n",
    "    # for label in tqdm(unique_targets):\n",
    "    #     for idx, target_name in enumerate(targets_list):\n",
    "    #         if label in target_name:\n",
    "    #             class_buckets[label][question_ids[idx]] = confidence[idx] # map the target to a question id and its associated confidence\n",
    "# class_buckets now contains all the labels mapped to a dictionary that contains question_ids:confidence where question_id's target has that label\n",
    "\n",
    "    # for label in unique_targets:\n",
    "    #     class_lengths[label] = len(class_buckets[label])\n",
    "    #class_buckets_sorted_length = dict(sorted(class_buckets.items(), key=lambda k: len(class_buckets[k])))# sort classes from least to greatest\n",
    "    class_buckets_sorted_length = OrderedDict(sorted(class_buckets.items(), key = lambda x : len(x[1])))\n",
    "    class_buckets_sorted_confidence = {}\n",
    "    for label in class_buckets_sorted_length:\n",
    "        class_buckets_sorted_confidence[label] = []\n",
    "    final_sample = []\n",
    "\n",
    "    for label in tqdm(class_buckets_sorted_length):\n",
    "        label_confidence_sorted = dict(sorted(class_buckets_sorted_length[label].items(), key=lambda x:x[1])) # sort each class by confidence\n",
    "        label_ids = list(label_confidence_sorted.keys())\n",
    "        label_ids_unique = list(set(label_ids).difference(set(final_sample)))\n",
    "\n",
    "        #class_buckets_sorted_confidence[label].extend(list(label_confidence_sorted.keys()))\n",
    "        class_buckets_sorted_confidence[label].extend(label_ids_unique)\n",
    "        \n",
    "        num_samples = round(len(class_buckets_sorted_confidence[label]) * (training_budget*0.01))\n",
    "        if len(class_buckets_sorted_confidence[label]) == 0:\n",
    "            num_samples = 0\n",
    "        else:\n",
    "            if num_samples == 0:\n",
    "                num_samples = 1\n",
    "\n",
    "        added_samples = 0\n",
    "        label_idx = 0\n",
    "        while added_samples <= num_samples and label_idx < len(class_buckets_sorted_confidence[label]):\n",
    "            added_samples +=1\n",
    "            if class_buckets_sorted_confidence[label][label_idx] not in final_sample:\n",
    "                added_samples +=1\n",
    "                final_sample.append(class_buckets_sorted_confidence[label][label_idx])  \n",
    "            label_idx+=1\n",
    "\n",
    "    total_samples = round((training_budget*0.01) * len(all_ids))\n",
    "    print(\"Total samples: \", total_samples)\n",
    "    print(\"sample size: \", len(set(final_sample)))\n",
    "    #final_sample = sample(final_sample, total_samples)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # for label in tqdm(class_buckets_sorted_confidence):\n",
    "    #     num_samples = round(len(class_buckets_sorted_confidence[label]) * (training_budget*0.01))\n",
    "    #     if num_samples == 0:\n",
    "    #         num_samples = 1\n",
    "    #     added_samples = 0\n",
    "    #     label_idx = 0\n",
    "    #     while added_samples <= num_samples and label_idx < len(class_buckets_sorted_confidence[label]):\n",
    "    #         if class_buckets_sorted_confidence[label][label_idx] not in final_sample:\n",
    "    #             final_sample.append(class_buckets_sorted_confidence[label][label_idx])\n",
    "    #             added_samples +=1\n",
    "    #         label_idx+=1\n",
    "\n",
    "    save_path = '../../../src/dataset_selection/sampling/samples/'+model+'/'+dataset+'/min_confidence/seed_'+str(965)+'/budget_'+str(training_budget)+'.pkl'\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(list(set(final_sample)), f)\n",
    "\n",
    "    unique_targets_sample = orig_df[orig_df['question_id'].isin(final_sample)]\n",
    "    sampled_targets_unique =[] # unique targets in vqa dataset \n",
    "    targets_list_sampled = np.array(unique_targets_sample['Target'].tolist())\n",
    "    for i in targets_list_sampled:\n",
    "        target_list = [x.strip() for x in i.split(',')]\n",
    "        sampled_targets_unique.extend(target_list)\n",
    "    unique_targets_sampled = set(sampled_targets_unique)\n",
    "\n",
    "    # sampled_targets_unique = [] # unique targets in sampled data\n",
    "    # sampled_targets = set(unique_targets_sample['Target'].unique())\n",
    "    # for sample in sampled_targets:\n",
    "    #     target_list = [x.strip() for x in sample.split(',')]\n",
    "    #     sampled_targets_unique.extend(target_list)\n",
    "    # sampled_targets_unique = set(sampled_targets_unique)\n",
    "    targets_excluded = unique_targets - unique_targets_sampled\n",
    "    print(\"TARGETS excluded: \", len(targets_excluded))\n",
    "\n",
    "    print(\"unique targets min confidence per class: \", len(set(unique_targets_sampled)))\n",
    "    print('samples - min confidence per class: ', len(set(final_sample)))\n",
    "    print('all_samples - min confidence per class: ', len(all_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "619390it [00:01, 436884.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3128/3128 [09:46<00:00,  5.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples:  198205\n",
      "sample size:  183431\n",
      "TARGETS excluded:  0\n",
      "unique targets min confidence per class:  3128\n",
      "samples - min confidence per class:  183431\n",
      "all_samples - min confidence per class:  619390\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampling_dataset = 'multilabel_full'\n",
    "sampling_model = 'LXR111'\n",
    "\n",
    "\n",
    "base_path = '../../../snap/vqa/lxr111_multilabel_full_run_3/'\n",
    "df = pd.read_pickle(base_path+\"datamap_metrics.pkl\")\n",
    "min_confidence_multilabel(df, sampling_model, 32, dataset=sampling_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_variability_multilabel(df, model, training_budget, dataset='animals'):\n",
    "    #df = df.sample(frac=0.5, replace=False, random_state=1)\n",
    "    targets = df['Target'].tolist()\n",
    "    targets = [i[0] for i in targets]\n",
    "    df['Target'] = targets\n",
    "    orig_df = df.copy()\n",
    "    all_ids = df['question_id'].to_list()\n",
    "\n",
    "    # sampled_question_ids = []\n",
    "    # sampled_variabilities = []\n",
    "    # sampled_confidence = []\n",
    "    # sampled_correctness = []\n",
    "    # sampled_targets = []\n",
    "\n",
    "    #unique_targets = df['Target'].unique()\n",
    "    targets_list = np.array(df['Target'].tolist())\n",
    "    question_ids = np.array(df['question_id'].to_list())\n",
    "    #confidence = np.array(df['confidence'].tolist())\n",
    "    variability = np.array(df['variability'].tolist())\n",
    "\n",
    "\n",
    "    targets_multilabel =[] # unique targets in vqa dataset \n",
    "    for i in targets_list:\n",
    "        target_list = [x.strip() for x in i.split(',')]\n",
    "        targets_multilabel.extend(target_list)\n",
    "    unique_targets = set(targets_multilabel)\n",
    "\n",
    "    class_buckets = {}\n",
    "    # class_lengths = {}\n",
    "    for label in unique_targets:\n",
    "        class_buckets[label] = {}\n",
    "        # class_lengths[label] = 0\n",
    "\n",
    "    for idx, target_name in tqdm(enumerate(targets_list)):\n",
    "        target_list = [x.strip() for x in target_name.split(',')]\n",
    "        for target_label in target_list:\n",
    "            class_buckets[target_label][question_ids[idx]] = variability[idx]\n",
    "    print(len(class_buckets))\n",
    "\n",
    "    \n",
    "    # for label in tqdm(unique_targets):\n",
    "    #     for idx, target_name in enumerate(targets_list):\n",
    "    #         if label in target_name:\n",
    "    #             class_buckets[label][question_ids[idx]] = confidence[idx] # map the target to a question id and its associated confidence\n",
    "# class_buckets now contains all the labels mapped to a dictionary that contains question_ids:confidence where question_id's target has that label\n",
    "\n",
    "    # for label in unique_targets:\n",
    "    #     class_lengths[label] = len(class_buckets[label])\n",
    "    #class_buckets_sorted_length = dict(sorted(class_buckets.items(), key=lambda k: len(class_buckets[k])))# sort classes from least to greatest\n",
    "    class_buckets_sorted_length = OrderedDict(sorted(class_buckets.items(), key = lambda x : len(x[1]), reverse=False))\n",
    "    class_buckets_sorted_variability = {}\n",
    "    for label in class_buckets_sorted_length:\n",
    "        class_buckets_sorted_variability[label] = []\n",
    "    final_sample = []\n",
    "\n",
    "    for label in tqdm(class_buckets_sorted_length):\n",
    "        label_variability_sorted = dict(sorted(class_buckets_sorted_length[label].items(), key=lambda x:x[1], reverse=True)) # sort each class by variability\n",
    "        label_ids = list(label_variability_sorted.keys())\n",
    "        label_ids_unique = list(set(label_ids).difference(set(final_sample)))\n",
    "\n",
    "        #class_buckets_sorted_confidence[label].extend(list(label_confidence_sorted.keys()))\n",
    "        class_buckets_sorted_variability[label].extend(label_ids_unique)\n",
    "        \n",
    "        num_samples = round(len(class_buckets_sorted_variability[label]) * (training_budget*0.01))\n",
    "        if len(class_buckets_sorted_variability[label]) == 0:\n",
    "            num_samples = 0\n",
    "        else:\n",
    "            if num_samples == 0:\n",
    "                num_samples = 1\n",
    "\n",
    "        added_samples = 0\n",
    "        label_idx = 0\n",
    "        while added_samples <= num_samples and label_idx < len(class_buckets_sorted_variability[label]):\n",
    "            added_samples +=1\n",
    "            if class_buckets_sorted_variability[label][label_idx] not in final_sample:\n",
    "                added_samples +=1\n",
    "                final_sample.append(class_buckets_sorted_variability[label][label_idx])  \n",
    "            label_idx+=1\n",
    "\n",
    "    total_samples = round((training_budget*0.01) * len(all_ids))\n",
    "    print(\"Total samples: \", total_samples)\n",
    "    print(\"sample size: \", len(set(final_sample)))\n",
    "    #final_sample = sample(final_sample, total_samples)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # for label in tqdm(class_buckets_sorted_confidence):\n",
    "    #     num_samples = round(len(class_buckets_sorted_confidence[label]) * (training_budget*0.01))\n",
    "    #     if num_samples == 0:\n",
    "    #         num_samples = 1\n",
    "    #     added_samples = 0\n",
    "    #     label_idx = 0\n",
    "    #     while added_samples <= num_samples and label_idx < len(class_buckets_sorted_confidence[label]):\n",
    "    #         if class_buckets_sorted_confidence[label][label_idx] not in final_sample:\n",
    "    #             final_sample.append(class_buckets_sorted_confidence[label][label_idx])\n",
    "    #             added_samples +=1\n",
    "    #         label_idx+=1\n",
    "\n",
    "    save_path = '../../../src/dataset_selection/sampling/samples/'+model+'/'+dataset+'/max_variability/seed_'+str(965)+'/budget_'+str(training_budget)+'.pkl'\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(list(set(final_sample)), f)\n",
    "\n",
    "    unique_targets_sample = orig_df[orig_df['question_id'].isin(final_sample)]\n",
    "    sampled_targets_unique =[] # unique targets in vqa dataset \n",
    "    targets_list_sampled = np.array(unique_targets_sample['Target'].tolist())\n",
    "    for i in targets_list_sampled:\n",
    "        target_list = [x.strip() for x in i.split(',')]\n",
    "        sampled_targets_unique.extend(target_list)\n",
    "    unique_targets_sampled = set(sampled_targets_unique)\n",
    "\n",
    "    # sampled_targets_unique = [] # unique targets in sampled data\n",
    "    # sampled_targets = set(unique_targets_sample['Target'].unique())\n",
    "    # for sample in sampled_targets:\n",
    "    #     target_list = [x.strip() for x in sample.split(',')]\n",
    "    #     sampled_targets_unique.extend(target_list)\n",
    "    # sampled_targets_unique = set(sampled_targets_unique)\n",
    "    targets_excluded = unique_targets - unique_targets_sampled\n",
    "    print(\"TARGETS excluded: \", len(targets_excluded))\n",
    "\n",
    "    print(\"unique targets max variability per class: \", len(set(unique_targets_sampled)))\n",
    "    print('samples - max variability per class: ', len(set(final_sample)))\n",
    "    print('all_samples - max variability per class: ', len(all_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "619390it [00:01, 440890.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3128/3128 [09:45<00:00,  5.34it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples:  198205\n",
      "sample size:  183451\n",
      "TARGETS excluded:  0\n",
      "unique targets max variability per class:  3128\n",
      "samples - max variability per class:  183451\n",
      "all_samples - max variability per class:  619390\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampling_dataset = 'multilabel_full'\n",
    "sampling_model = 'LXR111'\n",
    "\n",
    "\n",
    "base_path = '../../../snap/vqa/lxr111_multilabel_full_run_3/'\n",
    "df = pd.read_pickle(base_path+\"datamap_metrics.pkl\")\n",
    "max_variability_multilabel(df, sampling_model, 32, dataset=sampling_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lxmert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "408f321b22a65d7d3f121506280c03d9bcd580a47471795e8b6f8d70bb6ba487"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
