{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import math \n",
    "import collections\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import base64\n",
    "import itertools\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "sns.set(style='whitegrid', font_scale=1.6, font='Georgia', context='paper')\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "import pickle \n",
    "from sklearn.neighbors import KernelDensity\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def plot_stacked_region(base_path, sampling_base, sampling_ids_list):\n",
    "\n",
    "    region_counts = {\n",
    "        \"easy\": [],\n",
    "        \"ambiguous\": [],\n",
    "        \"hard\": [],\n",
    "    }\n",
    "\n",
    "    x_axis = []\n",
    "\n",
    "    df = pd.read_pickle(base_path+\"datamap_metrics.pkl\")\n",
    "\n",
    "    for sampled_question_ids_path in sampling_ids_list:\n",
    "        split_path = sampled_question_ids_path.split('/')\n",
    "        if 'beta_kernel' in sampled_question_ids_path:\n",
    "            sampling_name = split_path[2] + '_' + split_path[-1][:-14]\n",
    "        elif 'beta_pvals' in sampled_question_ids_path or 'beta_var_counts' in sampled_question_ids_path:\n",
    "            sampling_name = split_path[1] + '_' + split_path[-1][:-14]\n",
    "        elif 'global' in sampled_question_ids_path:\n",
    "            sampling_name = split_path[0]\n",
    "        else:\n",
    "            sampling_name = 'classwise_' + split_path[0]\n",
    "        x_axis.append(sampling_name)\n",
    "        \n",
    "        with open(sampled_question_ids_path, 'rb') as f:\n",
    "            sampled_ids = pickle.load(f)\n",
    "\n",
    "        df_sampled = df.loc[df['question_id'].isin(sampled_ids)]\n",
    "\n",
    "        df_easy = df_sampled.loc[(df_sampled['confidence'] > 0.6) & (df_sampled['variability'] < 0.15)]\n",
    "        df_hard = df_sampled.loc[(df_sampled['confidence'] < 0.4) & (df_sampled['variability'] < 0.2)]\n",
    "        df_ambiguous = df_sampled.loc[df_sampled['variability'] > 0.3]\n",
    "\n",
    "        region_counts['easy'].append(len(df_easy))\n",
    "        region_counts['hard'].append(len(df_hard))\n",
    "        region_counts['ambiguous'].append(len(df_ambiguous))\n",
    "    return region_counts, x_axis\n",
    "\n",
    "\n",
    "def plot_stacked_variability(base_path, sampling_base, sampling_ids_list):\n",
    "\n",
    "    region_counts = {\n",
    "        \"0-0.1\": [],\n",
    "        \"0.1-0.2\": [],\n",
    "        \"0.2-0.3\": [],\n",
    "        \"0.3-0.4\": [],\n",
    "        \"0.4-0.5\": [],\n",
    "    }\n",
    "\n",
    "    x_axis = []\n",
    "\n",
    "    df = pd.read_pickle(base_path+\"datamap_metrics.pkl\")\n",
    "\n",
    "    for sampled_question_ids_path in sampling_ids_list:\n",
    "        split_path = sampled_question_ids_path.split('/')\n",
    "        if 'beta_kernel' in sampled_question_ids_path:\n",
    "            sampling_name = split_path[2] + '_' + split_path[-1][:-14]\n",
    "        elif 'beta_pvals' in sampled_question_ids_path or 'beta_var_counts' in sampled_question_ids_path:\n",
    "            sampling_name = split_path[1] + '_' + split_path[-1][:-14]\n",
    "        elif 'global' in sampled_question_ids_path:\n",
    "            sampling_name = split_path[0]\n",
    "        else:\n",
    "            sampling_name = 'classwise_' + split_path[0]\n",
    "        x_axis.append(sampling_name)\n",
    "        \n",
    "        with open(sampled_question_ids_path, 'rb') as f:\n",
    "            sampled_ids = pickle.load(f)\n",
    "\n",
    "        df_sampled = df.loc[df['question_id'].isin(sampled_ids)]\n",
    "\n",
    "        df_one = df_sampled.loc[(df_sampled['variability'] > 0.0) & (df_sampled['variability'] <= 0.1)]\n",
    "        df_two = df_sampled.loc[(df_sampled['variability'] > 0.1) & (df_sampled['variability'] <= 0.2)]\n",
    "        df_three = df_sampled.loc[(df_sampled['variability'] > 0.2) & (df_sampled['variability'] <= 0.3)]\n",
    "        df_four = df_sampled.loc[(df_sampled['variability'] > 0.3) & (df_sampled['variability'] <= 0.4)]\n",
    "        df_five = df_sampled.loc[(df_sampled['variability'] > 0.4) & (df_sampled['variability'] <= 0.5)]\n",
    "\n",
    "        region_counts['0-0.1'].append(len(df_one))\n",
    "        region_counts['0.1-0.2'].append(len(df_two))\n",
    "        region_counts['0.2-0.3'].append(len(df_three))\n",
    "        region_counts['0.3-0.4'].append(len(df_four))\n",
    "        region_counts['0.4-0.5'].append(len(df_five))\n",
    "\n",
    "    return region_counts, x_axis\n",
    "\n",
    "def plot_stacked_confidence(base_path, sampling_base, sampling_ids_list):\n",
    "\n",
    "    region_counts = {\n",
    "        \"0-0.2\": [],\n",
    "        \"0.2-0.4\": [],\n",
    "        \"0.4-0.6\": [],\n",
    "        \"0.6-0.8\": [],\n",
    "        \"0.8-1.0\": [],\n",
    "    }\n",
    "\n",
    "    x_axis = []\n",
    "\n",
    "    df = pd.read_pickle(base_path+\"datamap_metrics.pkl\")\n",
    "\n",
    "    for sampled_question_ids_path in sampling_ids_list:\n",
    "        split_path = sampled_question_ids_path.split('/')\n",
    "        if 'beta_kernel' in sampled_question_ids_path:\n",
    "            sampling_name = split_path[2] + '_' + split_path[-1][:-14]\n",
    "        elif 'beta_pvals' in sampled_question_ids_path or 'beta_var_counts' in sampled_question_ids_path:\n",
    "            sampling_name = split_path[1] + '_' + split_path[-1][:-14]\n",
    "        elif 'global' in sampled_question_ids_path:\n",
    "            sampling_name = split_path[0]\n",
    "        else:\n",
    "            sampling_name = 'classwise_' + split_path[0]\n",
    "        x_axis.append(sampling_name)\n",
    "        \n",
    "        with open(sampled_question_ids_path, 'rb') as f:\n",
    "            sampled_ids = pickle.load(f)\n",
    "\n",
    "        df_sampled = df.loc[df['question_id'].isin(sampled_ids)]\n",
    "\n",
    "        df_one = df_sampled.loc[(df_sampled['confidence'] > 0.0) & (df_sampled['confidence'] <= 0.2)]\n",
    "        df_two = df_sampled.loc[(df_sampled['confidence'] > 0.2) & (df_sampled['confidence'] <= 0.4)]\n",
    "        df_three = df_sampled.loc[(df_sampled['confidence'] > 0.4) & (df_sampled['confidence'] <= 0.6)]\n",
    "        df_four = df_sampled.loc[(df_sampled['confidence'] > 0.6) & (df_sampled['confidence'] <= 0.8)]\n",
    "        df_five = df_sampled.loc[(df_sampled['confidence'] > 0.8) & (df_sampled['confidence'] <= 1.0)]\n",
    "\n",
    "        region_counts['0-0.2'].append(len(df_one))\n",
    "        region_counts['0.2-0.4'].append(len(df_two))\n",
    "        region_counts['0.4-0.6'].append(len(df_three))\n",
    "        region_counts['0.6-0.8'].append(len(df_four))\n",
    "        region_counts['0.8-1.0'].append(len(df_five))\n",
    "\n",
    "    return region_counts, x_axis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_path = '../../../snap/vqa/lxr111_multilabel_full_run_3/'\n",
    "sampling_base = '../../../src/dataset_selection/sampling/samples/LXR111/multilabel_full/'\n",
    "sampling_ids_list = ['beta/beta_kernel/tophat/seed_965/alpha_2_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/tophat/seed_965/alpha_2_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/tophat/seed_965/alpha_1_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/tophat/seed_965/alpha_1_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/cosine/seed_965/alpha_2_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/cosine/seed_965/alpha_2_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/cosine/seed_965/alpha_1_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/cosine/seed_965/alpha_1_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/epanechnikov/seed_965/alpha_2_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/epanechnikov/seed_965/alpha_2_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/epanechnikov/seed_965/alpha_1_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/epanechnikov/seed_965/alpha_1_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/exponential/seed_965/alpha_2_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/exponential/seed_965/alpha_2_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/exponential/seed_965/alpha_1_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/exponential/seed_965/alpha_1_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/gaussian/seed_965/alpha_2_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/gaussian/seed_965/alpha_2_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/gaussian/seed_965/alpha_1_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/gaussian/seed_965/alpha_1_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/linear/seed_965/alpha_2_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/linear/seed_965/alpha_2_beta_2_budget_30.pkl',\n",
    "                'beta/beta_kernel/linear/seed_965/alpha_1_beta_1_budget_30.pkl',\n",
    "                'beta/beta_kernel/linear/seed_965/alpha_1_beta_2_budget_30.pkl',\n",
    "                'beta/beta_pvals/seed_965/alpha_2_beta_1_budget_30.pkl',\n",
    "                'beta/beta_pvals/seed_965/alpha_2_beta_2_budget_30.pkl',\n",
    "                'beta/beta_pvals/seed_965/alpha_1_beta_1_budget_30.pkl',\n",
    "                'beta/beta_pvals/seed_965/alpha_1_beta_2_budget_30.pkl',\n",
    "                'beta/beta_var_counts/seed_965/alpha_2_beta_1_budget_30.pkl',\n",
    "                'beta/beta_var_counts/seed_965/alpha_2_beta_2_budget_30.pkl',\n",
    "                'beta/beta_var_counts/seed_965/alpha_1_beta_1_budget_30.pkl',\n",
    "                'beta/beta_var_counts/seed_965/alpha_1_beta_2_budget_30.pkl',\n",
    "                'global_max_confidence/seed_965/budget_30.pkl',\n",
    "                'global_max_variability/seed_965/budget_30.pkl',\n",
    "                'global_min_confidence/seed_965/budget_30.pkl',\n",
    "                'global_min_variability/seed_965/budget_30.pkl',\n",
    "                'global_random/seed_965/budget_30.pkl',\n",
    "                'max_confidence/seed_965/budget_30.pkl',\n",
    "                'max_variability/seed_965/budget_30.pkl',\n",
    "                'min_confidence/seed_965/budget_30.pkl',\n",
    "                'min_variability/seed_965/budget_30.pkl',\n",
    "                'random/seed_965/budget_30.pkl']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_counts_datamap, x_axis_datamap =  plot_stacked_region(base_path, sampling_base, sampling_ids_list)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(3)\n",
    "\n",
    "for boolean, weight_count in region_counts_datamap.items():\n",
    "    p = ax.bar(x_axis_datamap, weight_count, width, label=boolean, bottom=bottom)\n",
    "    bottom += weight_count\n",
    "\n",
    "ax.set_title(\"Datamap Region Frequency Across Selection Methods\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random sampling and beta sampling, get top question types like you did in test.ipynb and plot their binned confidence scores and binned variability scores\n",
    "\n",
    "\n",
    "# also calculate skew metrics comparing random and beta sampling class distributions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.7 (default, Jun 23 2021, 23:32:48) \n[GCC 7.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
