{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50178/3740038047.py:19: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import math \n",
    "import collections\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import base64\n",
    "import itertools\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "sns.set(style='whitegrid', font_scale=1.6, font='Georgia', context='paper')\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "import pickle \n",
    "from sklearn.neighbors import KernelDensity\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_confidence_multilabel(df, model, training_budget, dataset='animals'):\n",
    "    targets = df['Target'].tolist()\n",
    "    targets = [i[0] for i in targets]\n",
    "    df['Target'] = targets\n",
    "    orig_df = df.copy()\n",
    "    all_ids = df['question_id'].to_list()\n",
    "\n",
    "    # sampled_question_ids = []\n",
    "    # sampled_variabilities = []\n",
    "    # sampled_confidence = []\n",
    "    # sampled_correctness = []\n",
    "    # sampled_targets = []\n",
    "\n",
    "    #unique_targets = df['Target'].unique()\n",
    "    targets_list = np.array(df['Target'].tolist())\n",
    "    question_ids = np.array(df['question_id'].to_list())\n",
    "    confidence = np.array(df['confidence'].tolist())\n",
    "\n",
    "\n",
    "    targets_multilabel =[] # unique targets in vqa dataset \n",
    "    for i in targets_list:\n",
    "        target_list = [x.strip() for x in i.split(',')]\n",
    "        targets_multilabel.extend(target_list)\n",
    "    unique_targets = set(targets_multilabel)\n",
    "\n",
    "    class_buckets = {}\n",
    "    # class_lengths = {}\n",
    "    for label in unique_targets:\n",
    "        class_buckets[label] = {}\n",
    "        # class_lengths[label] = 0\n",
    "        \n",
    "    \n",
    "    \n",
    "    for label in tqdm(unique_targets):\n",
    "        for idx, target_name in enumerate(targets_list):\n",
    "            if label in target_name:\n",
    "                class_buckets[label][question_ids[idx]] = confidence[idx] # map the target to a question id and its associated confidence\n",
    "# class_buckets now contains all the labels mapped to a dictionary that contains question_ids:confidence where question_id's target has that label\n",
    "\n",
    "    # for label in unique_targets:\n",
    "    #     class_lengths[label] = len(class_buckets[label])\n",
    "    class_buckets_sorted_length = sorted(class_buckets, key=lambda k: len(class_buckets[k])) # sort classes from least to greatest\n",
    "    \n",
    "    class_buckets_sorted_confidence = {}\n",
    "    for label in class_buckets_sorted_length:\n",
    "        class_buckets_sorted_confidence[label] = []\n",
    "    for label in tqdm(class_buckets_sorted_length):\n",
    "        label_confidence_sorted = sorted(class_buckets_sorted_length[label].items(), key=lambda x:x[1]) # sort each class by confidence\n",
    "        class_buckets_sorted_confidence[label].extend(list(label_confidence_sorted.keys()))\n",
    "    \n",
    "\n",
    "    final_sample = []\n",
    "\n",
    "    for label in tqdm(class_buckets_sorted_confidence):\n",
    "        num_samples = round(len(class_buckets_sorted_confidence[label]) * (training_budget*0.01))\n",
    "        if num_samples == 0:\n",
    "            num_samples = 1\n",
    "        added_samples = 0\n",
    "        label_idx = 0\n",
    "        while added_samples <= num_samples and label_idx < len(class_buckets_sorted_confidence[label]):\n",
    "            if class_buckets_sorted_confidence[label][label_idx] not in final_sample:\n",
    "                final_sample.append(class_buckets_sorted_confidence[label][label_idx])\n",
    "                added_samples +=1\n",
    "            label_idx+=1\n",
    "\n",
    "    save_path = 'src/dataset_selection/sampling/samples/'+model+'/'+dataset+'/max_confidence/seed_'+str(965)+'/budget_'+str(training_budget)+'.pkl'\n",
    "\n",
    "\n",
    "    unique_targets_sample = orig_df[orig_df['question_id'].isin(final_sample)]\n",
    "    sampled_targets_unique =[] # unique targets in vqa dataset \n",
    "    targets_list_sampled = np.array(unique_targets_sample['Target'].tolist())\n",
    "    for i in targets_list_sampled:\n",
    "        target_list = [x.strip() for x in i.split(',')]\n",
    "        sampled_targets_unique.extend(target_list)\n",
    "    unique_targets_sampled = set(sampled_targets_unique)\n",
    "\n",
    "    # sampled_targets_unique = [] # unique targets in sampled data\n",
    "    # sampled_targets = set(unique_targets_sample['Target'].unique())\n",
    "    # for sample in sampled_targets:\n",
    "    #     target_list = [x.strip() for x in sample.split(',')]\n",
    "    #     sampled_targets_unique.extend(target_list)\n",
    "    # sampled_targets_unique = set(sampled_targets_unique)\n",
    "    targets_excluded = unique_targets - unique_targets_sampled\n",
    "    print(\"TARGETS excluded: \", len(targets_excluded))\n",
    "\n",
    "    \n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(list(set(sampled_question_ids)), f)\n",
    "\n",
    "    print(\"unique targets max confidence per class: \", len(set(unique_targets_sampled)))\n",
    "    print('samples - max confidence per class: ', len(set(final_sample)))\n",
    "    print('all_samples - max confidence per class: ', len(all_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\t\t  experience_in_pretraining.md\tREADME.md\t  src\n",
      "data\t\t  jupyterhub_cookie_secret\trequirements.txt  test.py\n",
      "datamaps\t  jupyterhub.sqlite\t\trun\n",
      "environment.yaml  LICENSE\t\t\tsnap\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 710/3128 [03:34<12:31,  3.22it/s]"
     ]
    }
   ],
   "source": [
    "base_path = '../../../snap/vqa/lxr111_multilabel_full_run_3/'\n",
    "df = pd.read_pickle(base_path+\"datamap_metrics.pkl\")\n",
    "sampling_dataset = 'multilabel_full'\n",
    "sampling_model = 'LXR111'\n",
    "max_confidence_multilabel(df, sampling_model, 30, dataset=sampling_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lxmert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "408f321b22a65d7d3f121506280c03d9bcd580a47471795e8b6f8d70bb6ba487"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
