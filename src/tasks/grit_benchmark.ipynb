{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa58519",
   "metadata": {},
   "source": [
    "* We have a vqa.json file that contains all the data to make predictions on the model\n",
    "* output: a vqa predictions file that we can submit to the leaderboard \n",
    "\n",
    "\n",
    "* Just make sure you can do normal validation first --- DONE\n",
    "* create Jupyter notebook that loads in model, and runs evaluation on a given image/question pair --- DONE\n",
    "* generate image features from faster-rcnn for visual genome and nyu\n",
    "* Given the vqa.json file from the grit benchmark, feed that into the model and get predictions\n",
    "* Calculate the score based on VQAâ€™s metric, so each sample in the vqa.json has 10 predictions and you need to manually get the score \n",
    "* Generate a predictions file \n",
    "* Do this for ablation and test split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5002441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyleft 2019 project LXRT.\n",
    "import neptune.new as neptune\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# from param import args\n",
    "from pretrain.qa_answer_table import load_lxmert_qa\n",
    "# from tasks.vqa_model import VQAModel\n",
    "# from tasks.vqa_data import VQADataset, VQATorchDataset, VQAEvaluator\n",
    "DataTuple = collections.namedtuple(\"DataTuple\", 'dataset loader evaluator')\n",
    "import json \n",
    "import os\n",
    "# from tasks.gqa_data import GQADataset, GQATorchDataset, GQAEvaluator\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c038b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9148d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "def get_optimizer(optim):\n",
    "    # Bind the optimizer\n",
    "    if optim == 'rms':\n",
    "        print(\"Optimizer: Using RMSProp\")\n",
    "        optimizer = torch.optim.RMSprop\n",
    "    elif optim == 'adam':\n",
    "        print(\"Optimizer: Using Adam\")\n",
    "        optimizer = torch.optim.Adam\n",
    "    elif optim == 'adamax':\n",
    "        print(\"Optimizer: Using Adamax\")\n",
    "        optimizer = torch.optim.Adamax\n",
    "    elif optim == 'sgd':\n",
    "        print(\"Optimizer: sgd\")\n",
    "        optimizer = torch.optim.SGD\n",
    "    elif 'bert' in optim:\n",
    "        optimizer = 'bert'      # The bert optimizer will be bind later.\n",
    "    else:\n",
    "        assert False, \"Please add your optimizer %s in the list.\" % optim\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data Splits\n",
    "    parser.add_argument(\"--train\", default='train')\n",
    "    parser.add_argument(\"--valid\", default='')\n",
    "    parser.add_argument(\"--test\", default='minival')\n",
    "    parser.add_argument(\"--subset\", type=str, default=None, help='vqa-animals, myo-sports, myo-animals')\n",
    "    parser.add_argument(\"--multiclass\", action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--multilabel\", action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--sampling_ids\", type=str, default=None) # TODO when using sampling ids with vqa_from_scratch.bash, change sampling args for appropriate neptune logging\n",
    "\n",
    "    # Training Hyper-parameters\n",
    "    parser.add_argument('--batchSize', dest='batch_size', type=int, default=256)\n",
    "    parser.add_argument('--optim', default='bert') \n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--dropout', type=float, default=0.1)\n",
    "    parser.add_argument('--seed', type=int, default=965, help='random seed')\n",
    "\n",
    "    # Debugging\n",
    "    parser.add_argument('--output', type=str, default='snap/test')\n",
    "    parser.add_argument(\"--fast\", action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--tiny\", action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--tqdm\", action='store_const', default=False, const=True)\n",
    "\n",
    "    # Model Loading\n",
    "    parser.add_argument('--load', type=str, default='/home/jaspreet/vl-pretraining/snap/vqa/lxr111_multilabel_full_run_3/BEST',\n",
    "                        help='Load the model (usually the fine-tuned model).')\n",
    "    parser.add_argument('--loadLXMERT', dest='load_lxmert', type=str, default=None,\n",
    "                        help='Load the pre-trained LXMERT model.')\n",
    "    parser.add_argument('--loadLXMERTQA', dest='load_lxmert_qa', type=str, default=None,\n",
    "                        help='Load the pre-trained LXMERT model with QA answer head.')\n",
    "    parser.add_argument(\"--fromScratch\", dest='from_scratch', action='store_const', default=False, const=True,\n",
    "                        help='If none of the --load, --loadLXMERT, --loadLXMERTQA is set, '\n",
    "                             'the model would be trained from scratch. If --fromScratch is'\n",
    "                             ' not specified, the model would load BERT-pre-trained weights by'\n",
    "                             ' default. ')\n",
    "\n",
    "    # Optimization\n",
    "    parser.add_argument(\"--mceLoss\", dest='mce_loss', action='store_const', default=False, const=True)\n",
    "\n",
    "    # LXRT Model Config\n",
    "    # Note: LXRT = L, X, R (three encoders), Transformer\n",
    "    parser.add_argument(\"--llayers\", default=1, type=int, help='Number of Language layers')\n",
    "    parser.add_argument(\"--xlayers\", default=1, type=int, help='Number of CROSS-modality layers.')\n",
    "    parser.add_argument(\"--rlayers\", default=1, type=int, help='Number of object Relationship layers.')\n",
    "\n",
    "    # LXMERT Pre-training Config\n",
    "    parser.add_argument(\"--taskMatched\", dest='task_matched', action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--taskMaskLM\", dest='task_mask_lm', action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--taskObjPredict\", dest='task_obj_predict', action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--taskQA\", dest='task_qa', action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--visualLosses\", dest='visual_losses', default='obj,attr,feat', type=str)\n",
    "    parser.add_argument(\"--qaSets\", dest='qa_sets', default=None, type=str)\n",
    "    parser.add_argument(\"--wordMaskRate\", dest='word_mask_rate', default=0.15, type=float)\n",
    "    parser.add_argument(\"--objMaskRate\", dest='obj_mask_rate', default=0.15, type=float)\n",
    "\n",
    "    # Training configuration\n",
    "    parser.add_argument(\"--multiGPU\", action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--numWorkers\", dest='num_workers', default=0)\n",
    "\n",
    "    # Datamaps\n",
    "    parser.add_argument(\"--base_path\", default='snap/vqa/lxr111_multilabel_full_run_3/', type=str, help='Path to trained model')\n",
    "    parser.add_argument(\"--datamap_title\", default='Trained from Scratch on VQA-Multiclass', type=str, help='Title of datamap plot')\n",
    "    parser.add_argument(\"--multilabel_datamaps\", action='store_const', default=True, const=False)\n",
    "\n",
    "    # Sampling\n",
    "    parser.add_argument(\"--sampling_method\", default='min_variability', type=str, help='Sampling algorithm - beta, random, max_variability, min_variability')\n",
    "    parser.add_argument(\"--sampling_model\", default='LXR111', type=str, help='Name of model you are sampling variability values from')\n",
    "    parser.add_argument(\"--training_budget\", default=30, type=int, help='Percentage of data sampled')\n",
    "    parser.add_argument(\"--sampling_dataset\", default='multilabel_full', type=str, help='animals, sports, myo-food, myo-sports, multilabel_full')\n",
    "    parser.add_argument(\"--include_all_classes\", action='store_const', default=False, const=True )\n",
    "    \n",
    "    # Beta sampling\n",
    "    parser.add_argument(\"--alpha\", default=2, type=int, help='alpha parameter for beta distribution')\n",
    "    parser.add_argument(\"--beta\", default=2, type=int, help='beta parameter for beta distribution')\n",
    "    parser.add_argument(\"--norm\", default='pvals', type=str, help='pvals, gaussian_kde, gaussian, tophat, epanechnikov, exponential, linear, cosine')\n",
    "    parser.add_argument(\"--bandwidth\", default=0.01, type=float, help='bandwidth for beta kernels')\n",
    "\n",
    "    # Optuna\n",
    "    parser.add_argument(\"--optuna_sweep\", default='other', type=str, help='beta, other')\n",
    "    parser.add_argument(\"--neptune_study_name\", default='global_min_variability', type=str, help='name of optuna study')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Parse the arguments.\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    # Bind optimizer class.\n",
    "    args.optimizer = get_optimizer(args.optim)\n",
    "\n",
    "    # Set seeds\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    print(args.seed)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a72dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vqa_model.py\n",
    "# coding=utf-8\n",
    "# Copyleft 2019 project LXRT.\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# from param import args\n",
    "from lxrt.entry import LXRTEncoder\n",
    "from lxrt.modeling import BertLayerNorm, GeLU\n",
    "\n",
    "# Max length including <bos> and <eos>\n",
    "MAX_VQA_LENGTH = 20\n",
    "\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, num_answers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build LXRT encoder\n",
    "        self.lxrt_encoder = LXRTEncoder(\n",
    "            args,\n",
    "            max_seq_length=MAX_VQA_LENGTH\n",
    "        )\n",
    "        hid_dim = self.lxrt_encoder.dim\n",
    "        \n",
    "        # VQA Answer heads\n",
    "        self.logit_fc = nn.Sequential(\n",
    "            nn.Linear(hid_dim, hid_dim * 2),\n",
    "            GeLU(),\n",
    "            BertLayerNorm(hid_dim * 2, eps=1e-12),\n",
    "            nn.Linear(hid_dim * 2, num_answers)\n",
    "        )\n",
    "        self.logit_fc.apply(self.lxrt_encoder.model.init_bert_weights)\n",
    "\n",
    "    def forward(self, feat, pos, sent):\n",
    "        \"\"\"\n",
    "        b -- batch_size, o -- object_number, f -- visual_feature_size\n",
    "\n",
    "        :param feat: (b, o, f)\n",
    "        :param pos:  (b, o, 4)\n",
    "        :param sent: (b,) Type -- list of string\n",
    "        :param leng: (b,) Type -- int numpy array\n",
    "        :return: (b, num_answer) The logit of each answers.\n",
    "        \"\"\"\n",
    "        x = self.lxrt_encoder(sent, (feat, pos))\n",
    "        logit = self.logit_fc(x)\n",
    "\n",
    "        return logit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe93aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vqa.py\n",
    "def get_data_tuple(splits: str, subset: str, bs:int, shuffle=False, drop_last=False, sampling_ids=None) -> DataTuple:\n",
    "    dset = VQADataset(splits, subset, sampling_ids)\n",
    "    if splits != 'minival':\n",
    "        index_dset = len(dset.data) % bs\n",
    "        if index_dset != 0:\n",
    "            dset.data = dset.data[:-index_dset] \n",
    "    tset = VQATorchDataset(dset)\n",
    "    if splits != 'minival':\n",
    "        index_tset = len(tset.data) % bs\n",
    "        if index_tset != 0:\n",
    "            tset.data = tset.data[:-index_tset]\n",
    "    evaluator = VQAEvaluator(dset)\n",
    "    data_loader = DataLoader(\n",
    "        tset, batch_size=bs,\n",
    "        shuffle=shuffle, num_workers=args.num_workers,\n",
    "        drop_last=drop_last, pin_memory=True\n",
    "    )\n",
    "    return DataTuple(dataset=dset, loader=data_loader, evaluator=evaluator)\n",
    "\n",
    "def get_gqa_tuple(splits: str, subset: str, bs:int, shuffle=False, drop_last=False) -> DataTuple:\n",
    "    dset = GQADataset(splits, subset)\n",
    "    tset = GQATorchDataset(dset)\n",
    "    evaluator = GQAEvaluator(dset)\n",
    "    data_loader = DataLoader(\n",
    "        tset, batch_size=bs,\n",
    "        shuffle=shuffle, num_workers=args.num_workers,\n",
    "        drop_last=drop_last, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return DataTuple(dataset=dset, loader=data_loader, evaluator=evaluator)\n",
    "\n",
    "class VQA:\n",
    "    def __init__(self, sampling_ids=None):\n",
    "        # Datasets\n",
    "        self.train_tuple = get_data_tuple(\n",
    "            args.train, args.subset, bs=args.batch_size, shuffle=True, drop_last=False, sampling_ids=sampling_ids\n",
    "        )\n",
    "\n",
    "        if args.valid != \"\":\n",
    "            self.valid_tuple = get_data_tuple(\n",
    "                args.valid, args.subset, bs=1024,\n",
    "                shuffle=False, drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            self.valid_tuple = None\n",
    "        \n",
    "        # Model\n",
    "        self.model = VQAModel(self.train_tuple.dataset.num_answers)\n",
    "\n",
    "        # Load pre-trained weights\n",
    "        if args.load_lxmert is not None:\n",
    "            self.model.lxrt_encoder.load(args.load_lxmert)\n",
    "        if args.load_lxmert_qa is not None:\n",
    "            load_lxmert_qa(args.load_lxmert_qa, self.model,\n",
    "                           label2ans=self.train_tuple.dataset.label2ans)\n",
    "        \n",
    "        # GPU options\n",
    "        self.model = self.model.cuda()\n",
    "        #self.model = self.model\n",
    "        if args.multiGPU:\n",
    "            self.model.lxrt_encoder.multi_gpu()\n",
    "\n",
    "        # Loss and Optimizer\n",
    "        if args.multiclass == True:\n",
    "            self.loss_fxn = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.loss_fxn = nn.BCEWithLogitsLoss()\n",
    "        if 'bert' in args.optim:\n",
    "            batch_per_epoch = len(self.train_tuple.loader)\n",
    "            t_total = int(batch_per_epoch * args.epochs)\n",
    "            print(\"BertAdam Total Iters: %d\" % t_total)\n",
    "            from lxrt.optimization import BertAdam\n",
    "            self.optim = BertAdam(list(self.model.parameters()),\n",
    "                                  lr=args.lr,\n",
    "                                  warmup=0.1,\n",
    "                                  t_total=t_total)\n",
    "        else:\n",
    "            self.optim = args.optimizer(self.model.parameters(), args.lr)\n",
    "        \n",
    "        # Output Directory\n",
    "        self.output = args.output\n",
    "        os.makedirs(self.output, exist_ok=True)\n",
    "\n",
    "    def train(self, train_tuple, eval_tuple):\n",
    "\n",
    "        dset, loader, evaluator = train_tuple\n",
    "        iter_wrapper = (lambda x: tqdm(x, total=len(loader))) if args.tqdm else (lambda x: x)\n",
    "\n",
    "        best_valid = 0.\n",
    "        all_loss = []\n",
    "        valid_scores = []\n",
    "        train_scores = []\n",
    "        training_stats = []\n",
    "        for epoch in range(args.epochs):\n",
    "            quesid2ans = {}\n",
    "\n",
    "            for i, (ques_id, feats, boxes, sent, target, img_id) in iter_wrapper(enumerate(loader)):\n",
    "\n",
    "                self.model.train()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                feats, boxes, target = feats.cuda(), boxes.cuda(), target.cuda()\n",
    "                logit = self.model(feats, boxes, sent)\n",
    "\n",
    "                #assert logit.dim() == target.dim() == 2\n",
    "                loss = self.loss_fxn(logit, target)\n",
    "                loss = loss * logit.size(1)\n",
    "                all_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), 5.)\n",
    "                self.optim.step()\n",
    "\n",
    "                if args.multiclass == True:\n",
    "                    softmax = torch.nn.Softmax(dim=1)\n",
    "                    logit_softmax = softmax(logit)\n",
    "                    gt_preds_probability_softmax = torch.squeeze(logit_softmax.gather(1, torch.unsqueeze(target, 1))) # Batchwise\n",
    "                    #score, label = logit_softmax.max(1)\n",
    "                    score, label = logit.max(1)\n",
    "                else:\n",
    "                    sigmoid = torch.nn.Sigmoid()\n",
    "                    logit_sigmoid = sigmoid(logit)\n",
    "                    score, label = logit.max(1) # gets the max predicted label for each instance \n",
    "                    target_bool = (target>0).long()\n",
    "                    gt_preds_probability_sigmoid = logit_sigmoid * target_bool \n",
    "\n",
    "                for qid, l in zip(ques_id, label.cpu().numpy()):\n",
    "                    ans = dset.label2ans[l]\n",
    "                    quesid2ans[qid.item()] = ans\n",
    "\n",
    "                if args.multiclass == True:\n",
    "                    for idx, question in enumerate(sent):\n",
    "                        preds = dset.label2ans[np.squeeze(label.cpu().numpy()[idx].astype(int))]\n",
    "                        ans_gt = dset.label2ans[np.squeeze(target.cpu().numpy()[idx].astype(int))]\n",
    "                        \n",
    "                        training_stats.append({\n",
    "                            \"Epoch\": int(epoch),\n",
    "                            \"Question ID\": int(ques_id[idx]),\n",
    "                            \"Image ID\": str(img_id[idx]),\n",
    "                            \"Question\": str(question),\n",
    "                            \"Target\": str(ans_gt),\n",
    "                            \"Prediction\": str(preds),\n",
    "                            \"GT Probability\": float(gt_preds_probability_softmax[idx])\n",
    "                            }\n",
    "                    )\n",
    "                        \n",
    "                    if i%1000 ==0:\n",
    "                        for idx, question in enumerate(sent):\n",
    "                            ans_gt = dset.label2ans[target.cpu().numpy()[idx]]\n",
    "                            preds = dset.label2ans[label.cpu().numpy()[idx]]\n",
    "                            preds_str = \"Image ID: \" + img_id[idx] + \"\\n Question: \" + question + \"\\n ans_gt: \" + ans_gt + \"\\n preds: \" + preds + \"\\n\"\n",
    "                            with open(self.output + \"/log_preds.log\", 'a') as preds_file:\n",
    "                                preds_file.write(preds_str)\n",
    "                                preds_file.flush()\n",
    "                else:\n",
    "                    for idx, question in enumerate(sent):\n",
    "                        preds = dset.label2ans[np.squeeze(label.cpu().numpy()[idx].astype(int))]\n",
    "                        target_numpy = target_bool.cpu().numpy()[idx]\n",
    "                        #print(\"target: \", target_numpy.shape)\n",
    "                        targets_indices = np.nonzero(target_numpy) # get indices of groundtruth \n",
    "                        #print(\"target indices: \", targets_indices)\n",
    "                        target_indices_list = []\n",
    "                        for i in targets_indices[0]:\n",
    "                            target_indices_list.append(i)\n",
    "                        #print(target_indices_list)\n",
    "\n",
    "                        all_ans_gt = []\n",
    "                        all_probs = []\n",
    "                        for target_idx in target_indices_list:\n",
    "                            #print(\"target idx: \", target_idx)\n",
    "                            all_ans_gt.append(dset.label2ans[target_idx])\n",
    "                        probs_sigmoid = gt_preds_probability_sigmoid.detach().cpu().numpy()[idx]\n",
    "                        \n",
    "                        probs = probs_sigmoid[np.nonzero(probs_sigmoid)]\n",
    "                        for x in probs:\n",
    "                            all_probs.append(str(x))\n",
    "\n",
    "                        #ans_gt = dset.label2ans[np.squeeze(target.cpu().numpy()[idx].astype(int))]\n",
    "                        # datum = dset.id2datum[ques_id[idx]]\n",
    "                        # answer_type = datum['answer_type']\n",
    "                        # question_type = datum['question_type']\n",
    "                        # label = datum['label']\n",
    "                        # score= 0.0\n",
    "                        # if preds in label:\n",
    "                        #     score += label[preds]\n",
    "\n",
    "\n",
    "                        training_stats.append({\n",
    "                            \"Epoch\": int(epoch),\n",
    "                            \"Question ID\": int(ques_id[idx]),\n",
    "                            \"Image ID\": str(img_id[idx]),\n",
    "                            \"Question\": str(question),\n",
    "                            \"Target\": ', '.join(all_ans_gt),\n",
    "                            \"Prediction\": str(preds),\n",
    "                            \"GT Probability\": ', '.join(all_probs)\n",
    "                            }\n",
    "                    )\n",
    "\n",
    "            log_str = \"\\nEpoch %d: Train %0.2f\\n\" % (epoch, evaluator.evaluate(quesid2ans) * 100.)\n",
    "            train_scores.append(evaluator.evaluate(quesid2ans) * 100.)\n",
    "\n",
    "            if self.valid_tuple is not None:  # Do Validation\n",
    "                valid_score = self.evaluate(eval_tuple)\n",
    "                valid_scores.append(valid_score)\n",
    "                if valid_score > best_valid:\n",
    "                    best_valid = valid_score\n",
    "                    self.save(\"BEST\")\n",
    "\n",
    "                log_str += \"Epoch %d: Valid %0.2f\\n\" % (epoch, valid_score * 100.) + \\\n",
    "                           \"Epoch %d: Best %0.2f\\n\" % (epoch, best_valid * 100.)\n",
    "\n",
    "            print(log_str, end='')\n",
    "\n",
    "            with open(self.output + \"/log.log\", 'a') as f:\n",
    "                f.write(log_str)\n",
    "                f.flush()\n",
    "\n",
    "        with open(self.output+'/datamaps_stats.json', 'w') as json_file:\n",
    "            json.dump(training_stats, json_file, \n",
    "                                indent=4,  \n",
    "                                separators=(',',': '))\n",
    "        self.save(\"LAST\")\n",
    "        #return best_valid * 100.\n",
    "\n",
    "    def predict(self, eval_tuple: DataTuple, train_label2ans=None, dump=None):\n",
    "        \"\"\"\n",
    "        Predict the answers to questions in a data split.\n",
    "\n",
    "        :param eval_tuple: The data tuple to be evaluated.\n",
    "        :param dump: The path of saved file to dump results.\n",
    "        :return: A dict of question_id to answer.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        dset, loader, evaluator = eval_tuple\n",
    "        quesid2ans = {}\n",
    "        for i, datum_tuple in enumerate(loader):\n",
    "            ques_id, feats, boxes, sent = datum_tuple[:4]   # Avoid seeing ground truth\n",
    "            with torch.no_grad():\n",
    "                feats, boxes = feats.cuda(), boxes.cuda()\n",
    "                logit = self.model(feats, boxes, sent)\n",
    "                if args.multiclass == True:\n",
    "                    softmax = torch.nn.Softmax()\n",
    "                    #score, label = softmax(logit).max(1)\n",
    "                    score, label = logit.max(1)\n",
    "                else:\n",
    "                    score, label = logit.max(1) # this will output predictions wrt the vqa classes\n",
    "                for qid, l in zip(ques_id, label.cpu().numpy()):\n",
    "                    if train_label2ans != None:\n",
    "                    #ans = dset.label2ans[l]\n",
    "                        ans = train_label2ans[l]\n",
    "                        quesid2ans[qid] = ans\n",
    "                    else:\n",
    "                        ans = dset.label2ans[l]\n",
    "                        quesid2ans[qid.item()] = ans\n",
    "        if dump is not None:\n",
    "            evaluator.dump_result(quesid2ans, dump)\n",
    "        return quesid2ans\n",
    "\n",
    "    def evaluate(self, eval_tuple: DataTuple, train_label2ans= None, dump=None):\n",
    "        \"\"\"Evaluate all data in data_tuple.\"\"\"\n",
    "\n",
    "        quesid2ans = self.predict(eval_tuple, train_label2ans=train_label2ans, dump=dump)\n",
    "        return eval_tuple.evaluator.evaluate(quesid2ans)\n",
    "\n",
    "    @staticmethod\n",
    "    def oracle_score(data_tuple):\n",
    "        dset, loader, evaluator = data_tuple\n",
    "        quesid2ans = {}\n",
    "        for i, (ques_id, feats, boxes, sent, target, img_id) in enumerate(loader):\n",
    "            if args.multiclass == True:\n",
    "                label = target\n",
    "            else:\n",
    "                _, label = target.max(1)\n",
    "            for qid, l in zip(ques_id, label.cpu().numpy()):\n",
    "                ans = dset.label2ans[l]\n",
    "                quesid2ans[qid.item()] = ans\n",
    "        return evaluator.evaluate(quesid2ans)\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(),\n",
    "                   os.path.join(self.output, \"%s.pth\" % name))\n",
    "\n",
    "    def load(self, path):\n",
    "        print(\"Load model from %s\" % path)\n",
    "        state_dict = torch.load(\"%s.pth\" % path)\n",
    "        self.model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ccf3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyleft 2019 project LXRT.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# from param import args\n",
    "from utils import load_obj_tsv\n",
    "\n",
    "# Load part of the dataset for fast checking.\n",
    "# Notice that here is the number of images instead of the number of data,\n",
    "# which means all related data to the images would be used.\n",
    "TINY_IMG_NUM = 512\n",
    "FAST_IMG_NUM = 5000\n",
    "\n",
    "# The path to data and image features.\n",
    "VQA_DATA_ROOT = '../../data/vqa/'\n",
    "MSCOCO_IMGFEAT_ROOT = '../../data/mscoco_imgfeat/'\n",
    "SPLIT2NAME = {\n",
    "    'train': 'train2014',\n",
    "    'valid': 'val2014',\n",
    "    'minival': 'val2014',\n",
    "    'nominival': 'val2014',\n",
    "    'test': 'test2015',\n",
    "}\n",
    "\n",
    "\n",
    "class VQADataset:\n",
    "    \"\"\"\n",
    "    A VQA data example in json file:\n",
    "        {\n",
    "            \"answer_type\": \"other\",\n",
    "            \"img_id\": \"COCO_train2014_000000458752\",\n",
    "            \"label\": {\n",
    "                \"net\": 1\n",
    "            },\n",
    "            \"question_id\": 458752000,\n",
    "            \"question_type\": \"what is this\",\n",
    "            \"sent\": \"What is this photo taken looking through?\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, splits: str, subset: str, sampling_ids: str):\n",
    "        self.name = splits\n",
    "        self.splits = splits.split(',')\n",
    "        self.subset = subset # training on subsets: multiclass \n",
    "        self.sampling_ids = sampling_ids\n",
    "\n",
    "        loaded_data = []\n",
    "        for split in self.splits:\n",
    "            loaded_data.extend(json.load(open(\"../../data/vqa/%s.json\" % split)))\n",
    "\n",
    "        # exclude examples with no labels\n",
    "        self.data = []\n",
    "        # Loading datasets, if no subset is specified, train on full dataset\n",
    "\n",
    "        print(\"Loading full multilabel classification dataset\")\n",
    "        if self.sampling_ids != None:\n",
    "            with open(self.sampling_ids, 'rb') as f:\n",
    "                self.sampled_ids = pickle.load(f)\n",
    "            print(\"ids length: \", len(self.sampled_ids))\n",
    "\n",
    "        # for datum in loaded_data:\n",
    "        #     # if 'label' in datum:\n",
    "        #     #     if len(datum['label']) > 0:\n",
    "        #     self.data.append(datum)\n",
    "\n",
    "        # Loading datasets\n",
    "        loaded_data = []\n",
    "        for split in self.splits:\n",
    "            loaded_data.extend(json.load(open(\"../../data/vqa/%s.json\" % split)))\n",
    "        #print(\"Load %d data from split(s) %s.\" % (len(self.data), self.name))\n",
    "        self.data = []\n",
    "\n",
    "        for datum in loaded_data:\n",
    "            if 'minival' in self.splits:\n",
    "                    self.data.append(datum)\n",
    "            else:\n",
    "                if self.sampling_ids != None:                            \n",
    "                    if datum['question_id'] in self.sampled_ids:\n",
    "                        self.data.append(datum)\n",
    "                else:\n",
    "                    self.data.append(datum)   \n",
    "\n",
    "        print(\"Load %d data from split(s) %s.\" % (len(self.data), self.name))\n",
    "\n",
    "        # Convert list to dict (for evaluation)\n",
    "        self.id2datum = {\n",
    "            datum['question_id']: datum\n",
    "            for datum in self.data\n",
    "        }\n",
    "\n",
    "        # Answers\n",
    "        self.ans2label = json.load(open(\"../../data/vqa/trainval_ans2label.json\"))\n",
    "        self.label2ans = json.load(open(\"../../data/vqa/trainval_label2ans.json\"))\n",
    "        assert len(self.ans2label) == len(self.label2ans)\n",
    "\n",
    "    @property\n",
    "    def num_answers(self):\n",
    "        return len(self.ans2label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "An example in obj36 tsv:\n",
    "FIELDNAMES = [\"img_id\", \"img_h\", \"img_w\", \"objects_id\", \"objects_conf\",\n",
    "              \"attrs_id\", \"attrs_conf\", \"num_boxes\", \"boxes\", \"features\"]\n",
    "FIELDNAMES would be keys in the dict returned by load_obj_tsv.\n",
    "\"\"\"\n",
    "class VQATorchDataset(Dataset):\n",
    "    def __init__(self, dataset: VQADataset):\n",
    "        super().__init__()\n",
    "        self.raw_dataset = dataset\n",
    "        if args.tiny:\n",
    "            topk = TINY_IMG_NUM\n",
    "        elif args.fast:\n",
    "            topk = FAST_IMG_NUM\n",
    "        else:\n",
    "            topk = None\n",
    "\n",
    "        # Loading detection features to img_data\n",
    "        img_data = []\n",
    "        for split in dataset.splits:\n",
    "            # Minival is 5K images in MS COCO, which is used in evaluating VQA/LXMERT-pre-training.\n",
    "            # It is saved as the top 5K features in val2014_***.tsv\n",
    "            load_topk = 5000 if (split == 'minival' and topk is None) else topk\n",
    "            img_data.extend(load_obj_tsv(\n",
    "                os.path.join(MSCOCO_IMGFEAT_ROOT, '%s_obj36.tsv' % (SPLIT2NAME[split])),\n",
    "                topk=load_topk))\n",
    "\n",
    "        # Convert img list to dict\n",
    "        self.imgid2img = {}\n",
    "        for img_datum in img_data:\n",
    "            self.imgid2img[img_datum['img_id']] = img_datum\n",
    "\n",
    "        # Only kept the data with loaded image features\n",
    "        self.data = []\n",
    "        for datum in self.raw_dataset.data:\n",
    "            if datum['img_id'] in self.imgid2img:\n",
    "                self.data.append(datum)\n",
    "        print(\"Use %d data in torch dataset\" % (len(self.data)))\n",
    "        print()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        datum = self.data[item]\n",
    "\n",
    "        # while datum['question_id'] in self.exclude_ids:\n",
    "        #     datum = self.data[item+1]\n",
    "\n",
    "\n",
    "        img_id = datum['img_id']\n",
    "        ques_id = datum['question_id']\n",
    "        ques = datum['sent']\n",
    "\n",
    "        # Get image info\n",
    "        img_info = self.imgid2img[img_id]\n",
    "        obj_num = img_info['num_boxes']\n",
    "        feats = img_info['features'].copy()\n",
    "        boxes = img_info['boxes'].copy()\n",
    "        assert obj_num == len(boxes) == len(feats)\n",
    "\n",
    "        # Normalize the boxes (to 0 ~ 1)\n",
    "        img_h, img_w = img_info['img_h'], img_info['img_w']\n",
    "        boxes = boxes.copy()\n",
    "        boxes[:, (0, 2)] /= img_w\n",
    "        boxes[:, (1, 3)] /= img_h\n",
    "        np.testing.assert_array_less(boxes, 1+1e-5)\n",
    "        np.testing.assert_array_less(-boxes, 0+1e-5)\n",
    "\n",
    "        # # Provide label (target)\n",
    "        # if 'label' in datum:\n",
    "        #     label = datum['label']\n",
    "        #     target = torch.zeros(self.raw_dataset.num_answers)\n",
    "        #     for ans, score in label.items():\n",
    "        #         target[self.raw_dataset.ans2label[ans]] = score\n",
    "        #     return ques_id, feats, boxes, ques, target\n",
    "        # else:\n",
    "        #     return ques_id, feats, boxes, ques\n",
    "        # Provide label (target)\n",
    "        if 'label' in datum:\n",
    "            label = datum['label']\n",
    "            target = torch.zeros(self.raw_dataset.num_answers)\n",
    "            if args.multiclass == True:\n",
    "                assert len(label) == 1 # ensure there is only one gold label\n",
    "                for ans, score in label.items():\n",
    "                    #if ans in self.raw_dataset.filtered: # double check the if answer is in filtered category\n",
    "                    target[self.raw_dataset.ans2label[ans]] = 1.0\n",
    "                target = torch.squeeze(target.nonzero())\n",
    "                target = target.long()\n",
    "            else:\n",
    "                for ans, score in label.items():\n",
    "                    target[self.raw_dataset.ans2label[ans]] = score\n",
    "            return ques_id, feats, boxes, ques, target, img_id\n",
    "        else:\n",
    "            return ques_id, feats, boxes, ques\n",
    "\n",
    "\n",
    "class VQAEvaluator:\n",
    "    def __init__(self, dataset: VQADataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def evaluate(self, quesid2ans: dict):\n",
    "        score = 0.\n",
    "        for quesid, ans in quesid2ans.items():\n",
    "            datum = self.dataset.id2datum[quesid]\n",
    "            label = datum['label']\n",
    "            if ans in label:\n",
    "                score += label[ans]\n",
    "        return score / len(quesid2ans)\n",
    "\n",
    "    def dump_result(self, quesid2ans: dict, path):\n",
    "        \"\"\"\n",
    "        Dump results to a json file, which could be submitted to the VQA online evaluation.\n",
    "        VQA json file submission requirement:\n",
    "            results = [result]\n",
    "            result = {\n",
    "                \"question_id\": int,\n",
    "                \"answer\": str\n",
    "            }\n",
    "\n",
    "        :param quesid2ans: dict of quesid --> ans\n",
    "        :param path: The desired path of saved file.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path, 'w') as f:\n",
    "            result = []\n",
    "            for ques_id, ans in quesid2ans.items():\n",
    "                datum = self.dataset.id2datum[ques_id]\n",
    "                answer_type = datum['answer_type']\n",
    "                img_id = datum['img_id']\n",
    "                label = datum['label']\n",
    "                question_type = datum['question_type']\n",
    "                question = datum['sent']\n",
    "                score = 0.\n",
    "                if ans in label:\n",
    "                    score += label[ans]\n",
    "                \n",
    "\n",
    "                result.append({\n",
    "                    'question_id': ques_id,\n",
    "                    'answer': ans,\n",
    "                    'answer_type': answer_type,\n",
    "                    'img_id': img_id,\n",
    "                    'label': label,\n",
    "                    'question_type': question_type,\n",
    "                    'question': question,\n",
    "                    'score': score\n",
    "                })\n",
    "            json.dump(result, f, indent=4, sort_keys=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2fec497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(occurences):\n",
    "    if occurences == 0:\n",
    "        return 0\n",
    "    elif occurences == 1:\n",
    "        return 0.3\n",
    "    elif occurences == 2:\n",
    "        return 0.6\n",
    "    elif occurences == 3:\n",
    "        return 0.9\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "372526c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full multilabel classification dataset\n",
      "Load 443757 data from split(s) train.\n",
      "Start to load Faster-RCNN detected objects from ../../data/mscoco_imgfeat/train2014_obj36.tsv\n",
      "Loaded 82783 images in file ../../data/mscoco_imgfeat/train2014_obj36.tsv in 328 seconds.\n",
      "Use 443648 data in torch dataset\n",
      "\n",
      "LXRT encoder with 1 l_layers, 1 x_layers, and 1 r_layers.\n",
      "BertAdam Total Iters: 17330\n"
     ]
    }
   ],
   "source": [
    "vqa = VQA()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4e8998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from /home/jaspreet/vl-pretraining/snap/vqa/lxr111_multilabel_full_run_3/BEST\n"
     ]
    }
   ],
   "source": [
    "if args.load is not None:\n",
    "    vqa.load(args.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa0cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full multilabel classification dataset\n",
      "Load 25994 data from split(s) minival.\n",
      "Start to load Faster-RCNN detected objects from ../../data/mscoco_imgfeat/val2014_obj36.tsv\n",
      "Loaded 5000 images in file ../../data/mscoco_imgfeat/val2014_obj36.tsv in 19 seconds.\n",
      "Use 25994 data in torch dataset\n",
      "\n",
      "0.6317380934061486\n"
     ]
    }
   ],
   "source": [
    "result = vqa.evaluate(\n",
    "    get_data_tuple('minival', args.subset, bs=950,\n",
    "                shuffle=False, drop_last=False)\n",
    ")\n",
    "# dump=os.path.join(args.output, 'minival_predict.json')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df8c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lxmert",
   "language": "python",
   "name": "lxmert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
