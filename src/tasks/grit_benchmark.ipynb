{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa58519",
   "metadata": {},
   "source": [
    "* We have a vqa.json file that contains all the data to make predictions on the model\n",
    "* output: a vqa predictions file that we can submit to the leaderboard \n",
    "\n",
    "\n",
    "* Just make sure you can do normal validation first \n",
    "* create Jupyter notebook that loads in model, and runs evaluation on a given image/question pair \n",
    "* Given the vqa.json file from the grit benchmark, feed that into the model and get predictions\n",
    "* Calculate the score based on VQAâ€™s metric, so each sample in the vqa.json has 10 predictions and you need to manually get the score \n",
    "* Generate a predictions file \n",
    "* Do this for ablation and test split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5002441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyleft 2019 project LXRT.\n",
    "import neptune.new as neptune\n",
    "\n",
    "import os\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# from param import args\n",
    "from pretrain.qa_answer_table import load_lxmert_qa\n",
    "# from tasks.vqa_model import VQAModel\n",
    "# from tasks.vqa_data import VQADataset, VQATorchDataset, VQAEvaluator\n",
    "DataTuple = collections.namedtuple(\"DataTuple\", 'dataset loader evaluator')\n",
    "import json \n",
    "import os\n",
    "# from tasks.gqa_data import GQADataset, GQATorchDataset, GQAEvaluator\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce9148d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "def get_optimizer(optim):\n",
    "    # Bind the optimizer\n",
    "    if optim == 'rms':\n",
    "        print(\"Optimizer: Using RMSProp\")\n",
    "        optimizer = torch.optim.RMSprop\n",
    "    elif optim == 'adam':\n",
    "        print(\"Optimizer: Using Adam\")\n",
    "        optimizer = torch.optim.Adam\n",
    "    elif optim == 'adamax':\n",
    "        print(\"Optimizer: Using Adamax\")\n",
    "        optimizer = torch.optim.Adamax\n",
    "    elif optim == 'sgd':\n",
    "        print(\"Optimizer: sgd\")\n",
    "        optimizer = torch.optim.SGD\n",
    "    elif 'bert' in optim:\n",
    "        optimizer = 'bert'      # The bert optimizer will be bind later.\n",
    "    else:\n",
    "        assert False, \"Please add your optimizer %s in the list.\" % optim\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data Splits\n",
    "    parser.add_argument(\"--train\", default='train')\n",
    "    parser.add_argument(\"--valid\", default='')\n",
    "    parser.add_argument(\"--test\", default='minival')\n",
    "    parser.add_argument(\"--subset\", type=str, default=None, help='vqa-animals, myo-sports, myo-animals')\n",
    "    parser.add_argument(\"--multiclass\", action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--multilabel\", action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--sampling_ids\", type=str, default=None) # TODO when using sampling ids with vqa_from_scratch.bash, change sampling args for appropriate neptune logging\n",
    "\n",
    "    # Training Hyper-parameters\n",
    "    parser.add_argument('--batchSize', dest='batch_size', type=int, default=256)\n",
    "    parser.add_argument('--optim', default='bert') \n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--dropout', type=float, default=0.1)\n",
    "    parser.add_argument('--seed', type=int, default=965, help='random seed')\n",
    "\n",
    "    # Debugging\n",
    "    parser.add_argument('--output', type=str, default='snap/test')\n",
    "    parser.add_argument(\"--fast\", action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--tiny\", action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--tqdm\", action='store_const', default=False, const=True)\n",
    "\n",
    "    # Model Loading\n",
    "    parser.add_argument('--load', type=str, default='/home/jaspreet/vl-pretraining/snajaspreet/vl-pretraining/snap/vqa/lxr111_multilabel_full_run_3/BEST',\n",
    "                        help='Load the model (usually the fine-tuned model).')\n",
    "    parser.add_argument('--loadLXMERT', dest='load_lxmert', type=str, default=None,\n",
    "                        help='Load the pre-trained LXMERT model.')\n",
    "    parser.add_argument('--loadLXMERTQA', dest='load_lxmert_qa', type=str, default=None,\n",
    "                        help='Load the pre-trained LXMERT model with QA answer head.')\n",
    "    parser.add_argument(\"--fromScratch\", dest='from_scratch', action='store_const', default=False, const=True,\n",
    "                        help='If none of the --load, --loadLXMERT, --loadLXMERTQA is set, '\n",
    "                             'the model would be trained from scratch. If --fromScratch is'\n",
    "                             ' not specified, the model would load BERT-pre-trained weights by'\n",
    "                             ' default. ')\n",
    "\n",
    "    # Optimization\n",
    "    parser.add_argument(\"--mceLoss\", dest='mce_loss', action='store_const', default=False, const=True)\n",
    "\n",
    "    # LXRT Model Config\n",
    "    # Note: LXRT = L, X, R (three encoders), Transformer\n",
    "    parser.add_argument(\"--llayers\", default=1, type=int, help='Number of Language layers')\n",
    "    parser.add_argument(\"--xlayers\", default=1, type=int, help='Number of CROSS-modality layers.')\n",
    "    parser.add_argument(\"--rlayers\", default=1, type=int, help='Number of object Relationship layers.')\n",
    "\n",
    "    # LXMERT Pre-training Config\n",
    "    parser.add_argument(\"--taskMatched\", dest='task_matched', action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--taskMaskLM\", dest='task_mask_lm', action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--taskObjPredict\", dest='task_obj_predict', action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--taskQA\", dest='task_qa', action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--visualLosses\", dest='visual_losses', default='obj,attr,feat', type=str)\n",
    "    parser.add_argument(\"--qaSets\", dest='qa_sets', default=None, type=str)\n",
    "    parser.add_argument(\"--wordMaskRate\", dest='word_mask_rate', default=0.15, type=float)\n",
    "    parser.add_argument(\"--objMaskRate\", dest='obj_mask_rate', default=0.15, type=float)\n",
    "\n",
    "    # Training configuration\n",
    "    parser.add_argument(\"--multiGPU\", action='store_const', default=False, const=True)\n",
    "    parser.add_argument(\"--numWorkers\", dest='num_workers', default=0)\n",
    "\n",
    "    # Datamaps\n",
    "    parser.add_argument(\"--base_path\", default='snap/vqa/lxr111_multilabel_full_run_3/', type=str, help='Path to trained model')\n",
    "    parser.add_argument(\"--datamap_title\", default='Trained from Scratch on VQA-Multiclass', type=str, help='Title of datamap plot')\n",
    "    parser.add_argument(\"--multilabel_datamaps\", action='store_const', default=True, const=False)\n",
    "\n",
    "    # Sampling\n",
    "    parser.add_argument(\"--sampling_method\", default='min_variability', type=str, help='Sampling algorithm - beta, random, max_variability, min_variability')\n",
    "    parser.add_argument(\"--sampling_model\", default='LXR111', type=str, help='Name of model you are sampling variability values from')\n",
    "    parser.add_argument(\"--training_budget\", default=30, type=int, help='Percentage of data sampled')\n",
    "    parser.add_argument(\"--sampling_dataset\", default='multilabel_full', type=str, help='animals, sports, myo-food, myo-sports, multilabel_full')\n",
    "    parser.add_argument(\"--include_all_classes\", action='store_const', default=False, const=True )\n",
    "    \n",
    "    # Beta sampling\n",
    "    parser.add_argument(\"--alpha\", default=2, type=int, help='alpha parameter for beta distribution')\n",
    "    parser.add_argument(\"--beta\", default=2, type=int, help='beta parameter for beta distribution')\n",
    "    parser.add_argument(\"--norm\", default='pvals', type=str, help='pvals, gaussian_kde, gaussian, tophat, epanechnikov, exponential, linear, cosine')\n",
    "    parser.add_argument(\"--bandwidth\", default=0.01, type=float, help='bandwidth for beta kernels')\n",
    "\n",
    "    # Optuna\n",
    "    parser.add_argument(\"--optuna_sweep\", default='other', type=str, help='beta, other')\n",
    "    parser.add_argument(\"--neptune_study_name\", default='global_min_variability', type=str, help='name of optuna study')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Parse the arguments.\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    # Bind optimizer class.\n",
    "    args.optimizer = get_optimizer(args.optim)\n",
    "\n",
    "    # Set seeds\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    print(args.seed)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12a72dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vqa_model.py\n",
    "# coding=utf-8\n",
    "# Copyleft 2019 project LXRT.\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# from param import args\n",
    "from lxrt.entry import LXRTEncoder\n",
    "from lxrt.modeling import BertLayerNorm, GeLU\n",
    "\n",
    "# Max length including <bos> and <eos>\n",
    "MAX_VQA_LENGTH = 20\n",
    "\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, num_answers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build LXRT encoder\n",
    "        self.lxrt_encoder = LXRTEncoder(\n",
    "            args,\n",
    "            max_seq_length=MAX_VQA_LENGTH\n",
    "        )\n",
    "        hid_dim = self.lxrt_encoder.dim\n",
    "        \n",
    "        # VQA Answer heads\n",
    "        self.logit_fc = nn.Sequential(\n",
    "            nn.Linear(hid_dim, hid_dim * 2),\n",
    "            GeLU(),\n",
    "            BertLayerNorm(hid_dim * 2, eps=1e-12),\n",
    "            nn.Linear(hid_dim * 2, num_answers)\n",
    "        )\n",
    "        self.logit_fc.apply(self.lxrt_encoder.model.init_bert_weights)\n",
    "\n",
    "    def forward(self, feat, pos, sent):\n",
    "        \"\"\"\n",
    "        b -- batch_size, o -- object_number, f -- visual_feature_size\n",
    "\n",
    "        :param feat: (b, o, f)\n",
    "        :param pos:  (b, o, 4)\n",
    "        :param sent: (b,) Type -- list of string\n",
    "        :param leng: (b,) Type -- int numpy array\n",
    "        :return: (b, num_answer) The logit of each answers.\n",
    "        \"\"\"\n",
    "        x = self.lxrt_encoder(sent, (feat, pos))\n",
    "        logit = self.logit_fc(x)\n",
    "\n",
    "        return logit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe93aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vqa.py\n",
    "def get_data_tuple(splits: str, subset: str, bs:int, shuffle=False, drop_last=False, sampling_ids=None) -> DataTuple:\n",
    "    dset = VQADataset(splits, subset, sampling_ids)\n",
    "    if splits != 'minival':\n",
    "        index_dset = len(dset.data) % bs\n",
    "        if index_dset != 0:\n",
    "            dset.data = dset.data[:-index_dset] \n",
    "    tset = VQATorchDataset(dset)\n",
    "    if splits != 'minival':\n",
    "        index_tset = len(tset.data) % bs\n",
    "        if index_tset != 0:\n",
    "            tset.data = tset.data[:-index_tset]\n",
    "    evaluator = VQAEvaluator(dset)\n",
    "    data_loader = DataLoader(\n",
    "        tset, batch_size=bs,\n",
    "        shuffle=shuffle, num_workers=args.num_workers,\n",
    "        drop_last=drop_last, pin_memory=True\n",
    "    )\n",
    "    return DataTuple(dataset=dset, loader=data_loader, evaluator=evaluator)\n",
    "\n",
    "def get_gqa_tuple(splits: str, subset: str, bs:int, shuffle=False, drop_last=False) -> DataTuple:\n",
    "    dset = GQADataset(splits, subset)\n",
    "    tset = GQATorchDataset(dset)\n",
    "    evaluator = GQAEvaluator(dset)\n",
    "    data_loader = DataLoader(\n",
    "        tset, batch_size=bs,\n",
    "        shuffle=shuffle, num_workers=args.num_workers,\n",
    "        drop_last=drop_last, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return DataTuple(dataset=dset, loader=data_loader, evaluator=evaluator)\n",
    "\n",
    "class VQA:\n",
    "    def __init__(self, sampling_ids=None):\n",
    "        # Datasets\n",
    "        self.train_tuple = get_data_tuple(\n",
    "            args.train, args.subset, bs=args.batch_size, shuffle=True, drop_last=False, sampling_ids=sampling_ids\n",
    "        )\n",
    "\n",
    "        if args.valid != \"\":\n",
    "            self.valid_tuple = get_data_tuple(\n",
    "                args.valid, args.subset, bs=1024,\n",
    "                shuffle=False, drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            self.valid_tuple = None\n",
    "        \n",
    "        # Model\n",
    "        self.model = VQAModel(self.train_tuple.dataset.num_answers)\n",
    "\n",
    "        # Load pre-trained weights\n",
    "        if args.load_lxmert is not None:\n",
    "            self.model.lxrt_encoder.load(args.load_lxmert)\n",
    "        if args.load_lxmert_qa is not None:\n",
    "            load_lxmert_qa(args.load_lxmert_qa, self.model,\n",
    "                           label2ans=self.train_tuple.dataset.label2ans)\n",
    "        \n",
    "        # GPU options\n",
    "        self.model = self.model.cuda()\n",
    "        if args.multiGPU:\n",
    "            self.model.lxrt_encoder.multi_gpu()\n",
    "\n",
    "        # Loss and Optimizer\n",
    "        if args.multiclass == True:\n",
    "            self.loss_fxn = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.loss_fxn = nn.BCEWithLogitsLoss()\n",
    "        if 'bert' in args.optim:\n",
    "            batch_per_epoch = len(self.train_tuple.loader)\n",
    "            t_total = int(batch_per_epoch * args.epochs)\n",
    "            print(\"BertAdam Total Iters: %d\" % t_total)\n",
    "            from lxrt.optimization import BertAdam\n",
    "            self.optim = BertAdam(list(self.model.parameters()),\n",
    "                                  lr=args.lr,\n",
    "                                  warmup=0.1,\n",
    "                                  t_total=t_total)\n",
    "        else:\n",
    "            self.optim = args.optimizer(self.model.parameters(), args.lr)\n",
    "        \n",
    "        # Output Directory\n",
    "        self.output = args.output\n",
    "        os.makedirs(self.output, exist_ok=True)\n",
    "\n",
    "    def train(self, train_tuple, eval_tuple):\n",
    "\n",
    "        dset, loader, evaluator = train_tuple\n",
    "        iter_wrapper = (lambda x: tqdm(x, total=len(loader))) if args.tqdm else (lambda x: x)\n",
    "\n",
    "        best_valid = 0.\n",
    "        all_loss = []\n",
    "        valid_scores = []\n",
    "        train_scores = []\n",
    "        training_stats = []\n",
    "        for epoch in range(args.epochs):\n",
    "            quesid2ans = {}\n",
    "\n",
    "            for i, (ques_id, feats, boxes, sent, target, img_id) in iter_wrapper(enumerate(loader)):\n",
    "\n",
    "                self.model.train()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                feats, boxes, target = feats.cuda(), boxes.cuda(), target.cuda()\n",
    "                logit = self.model(feats, boxes, sent)\n",
    "\n",
    "                #assert logit.dim() == target.dim() == 2\n",
    "                loss = self.loss_fxn(logit, target)\n",
    "                loss = loss * logit.size(1)\n",
    "                all_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), 5.)\n",
    "                self.optim.step()\n",
    "\n",
    "                if args.multiclass == True:\n",
    "                    softmax = torch.nn.Softmax(dim=1)\n",
    "                    logit_softmax = softmax(logit)\n",
    "                    gt_preds_probability_softmax = torch.squeeze(logit_softmax.gather(1, torch.unsqueeze(target, 1))) # Batchwise\n",
    "                    #score, label = logit_softmax.max(1)\n",
    "                    score, label = logit.max(1)\n",
    "                else:\n",
    "                    sigmoid = torch.nn.Sigmoid()\n",
    "                    logit_sigmoid = sigmoid(logit)\n",
    "                    score, label = logit.max(1) # gets the max predicted label for each instance \n",
    "                    target_bool = (target>0).long()\n",
    "                    gt_preds_probability_sigmoid = logit_sigmoid * target_bool \n",
    "\n",
    "                for qid, l in zip(ques_id, label.cpu().numpy()):\n",
    "                    ans = dset.label2ans[l]\n",
    "                    quesid2ans[qid.item()] = ans\n",
    "\n",
    "                if args.multiclass == True:\n",
    "                    for idx, question in enumerate(sent):\n",
    "                        preds = dset.label2ans[np.squeeze(label.cpu().numpy()[idx].astype(int))]\n",
    "                        ans_gt = dset.label2ans[np.squeeze(target.cpu().numpy()[idx].astype(int))]\n",
    "                        \n",
    "                        training_stats.append({\n",
    "                            \"Epoch\": int(epoch),\n",
    "                            \"Question ID\": int(ques_id[idx]),\n",
    "                            \"Image ID\": str(img_id[idx]),\n",
    "                            \"Question\": str(question),\n",
    "                            \"Target\": str(ans_gt),\n",
    "                            \"Prediction\": str(preds),\n",
    "                            \"GT Probability\": float(gt_preds_probability_softmax[idx])\n",
    "                            }\n",
    "                    )\n",
    "                        \n",
    "                    if i%1000 ==0:\n",
    "                        for idx, question in enumerate(sent):\n",
    "                            ans_gt = dset.label2ans[target.cpu().numpy()[idx]]\n",
    "                            preds = dset.label2ans[label.cpu().numpy()[idx]]\n",
    "                            preds_str = \"Image ID: \" + img_id[idx] + \"\\n Question: \" + question + \"\\n ans_gt: \" + ans_gt + \"\\n preds: \" + preds + \"\\n\"\n",
    "                            with open(self.output + \"/log_preds.log\", 'a') as preds_file:\n",
    "                                preds_file.write(preds_str)\n",
    "                                preds_file.flush()\n",
    "                else:\n",
    "                    for idx, question in enumerate(sent):\n",
    "                        preds = dset.label2ans[np.squeeze(label.cpu().numpy()[idx].astype(int))]\n",
    "                        target_numpy = target_bool.cpu().numpy()[idx]\n",
    "                        #print(\"target: \", target_numpy.shape)\n",
    "                        targets_indices = np.nonzero(target_numpy) # get indices of groundtruth \n",
    "                        #print(\"target indices: \", targets_indices)\n",
    "                        target_indices_list = []\n",
    "                        for i in targets_indices[0]:\n",
    "                            target_indices_list.append(i)\n",
    "                        #print(target_indices_list)\n",
    "\n",
    "                        all_ans_gt = []\n",
    "                        all_probs = []\n",
    "                        for target_idx in target_indices_list:\n",
    "                            #print(\"target idx: \", target_idx)\n",
    "                            all_ans_gt.append(dset.label2ans[target_idx])\n",
    "                        probs_sigmoid = gt_preds_probability_sigmoid.detach().cpu().numpy()[idx]\n",
    "                        \n",
    "                        probs = probs_sigmoid[np.nonzero(probs_sigmoid)]\n",
    "                        for x in probs:\n",
    "                            all_probs.append(str(x))\n",
    "\n",
    "                        #ans_gt = dset.label2ans[np.squeeze(target.cpu().numpy()[idx].astype(int))]\n",
    "                        # datum = dset.id2datum[ques_id[idx]]\n",
    "                        # answer_type = datum['answer_type']\n",
    "                        # question_type = datum['question_type']\n",
    "                        # label = datum['label']\n",
    "                        # score= 0.0\n",
    "                        # if preds in label:\n",
    "                        #     score += label[preds]\n",
    "\n",
    "\n",
    "                        training_stats.append({\n",
    "                            \"Epoch\": int(epoch),\n",
    "                            \"Question ID\": int(ques_id[idx]),\n",
    "                            \"Image ID\": str(img_id[idx]),\n",
    "                            \"Question\": str(question),\n",
    "                            \"Target\": ', '.join(all_ans_gt),\n",
    "                            \"Prediction\": str(preds),\n",
    "                            \"GT Probability\": ', '.join(all_probs)\n",
    "                            }\n",
    "                    )\n",
    "\n",
    "            log_str = \"\\nEpoch %d: Train %0.2f\\n\" % (epoch, evaluator.evaluate(quesid2ans) * 100.)\n",
    "            train_scores.append(evaluator.evaluate(quesid2ans) * 100.)\n",
    "\n",
    "            if self.valid_tuple is not None:  # Do Validation\n",
    "                valid_score = self.evaluate(eval_tuple)\n",
    "                valid_scores.append(valid_score)\n",
    "                if valid_score > best_valid:\n",
    "                    best_valid = valid_score\n",
    "                    self.save(\"BEST\")\n",
    "\n",
    "                log_str += \"Epoch %d: Valid %0.2f\\n\" % (epoch, valid_score * 100.) + \\\n",
    "                           \"Epoch %d: Best %0.2f\\n\" % (epoch, best_valid * 100.)\n",
    "\n",
    "            print(log_str, end='')\n",
    "\n",
    "            with open(self.output + \"/log.log\", 'a') as f:\n",
    "                f.write(log_str)\n",
    "                f.flush()\n",
    "\n",
    "        with open(self.output+'/datamaps_stats.json', 'w') as json_file:\n",
    "            json.dump(training_stats, json_file, \n",
    "                                indent=4,  \n",
    "                                separators=(',',': '))\n",
    "        self.save(\"LAST\")\n",
    "        #return best_valid * 100.\n",
    "\n",
    "    def predict(self, eval_tuple: DataTuple, train_label2ans=None, dump=None):\n",
    "        \"\"\"\n",
    "        Predict the answers to questions in a data split.\n",
    "\n",
    "        :param eval_tuple: The data tuple to be evaluated.\n",
    "        :param dump: The path of saved file to dump results.\n",
    "        :return: A dict of question_id to answer.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        dset, loader, evaluator = eval_tuple\n",
    "        quesid2ans = {}\n",
    "        for i, datum_tuple in enumerate(loader):\n",
    "            ques_id, feats, boxes, sent = datum_tuple[:4]   # Avoid seeing ground truth\n",
    "            with torch.no_grad():\n",
    "                feats, boxes = feats.cuda(), boxes.cuda()\n",
    "                logit = self.model(feats, boxes, sent)\n",
    "                if args.multiclass == True:\n",
    "                    softmax = torch.nn.Softmax()\n",
    "                    #score, label = softmax(logit).max(1)\n",
    "                    score, label = logit.max(1)\n",
    "                else:\n",
    "                    score, label = logit.max(1) # this will output predictions wrt the vqa classes\n",
    "                for qid, l in zip(ques_id, label.cpu().numpy()):\n",
    "                    if train_label2ans != None:\n",
    "                    #ans = dset.label2ans[l]\n",
    "                        ans = train_label2ans[l]\n",
    "                        quesid2ans[qid] = ans\n",
    "                    else:\n",
    "                        ans = dset.label2ans[l]\n",
    "                        quesid2ans[qid.item()] = ans\n",
    "        if dump is not None:\n",
    "            evaluator.dump_result(quesid2ans, dump)\n",
    "        return quesid2ans\n",
    "\n",
    "    def evaluate(self, eval_tuple: DataTuple, train_label2ans= None, dump=None):\n",
    "        \"\"\"Evaluate all data in data_tuple.\"\"\"\n",
    "\n",
    "        quesid2ans = self.predict(eval_tuple, train_label2ans=train_label2ans, dump=dump)\n",
    "        return eval_tuple.evaluator.evaluate(quesid2ans)\n",
    "\n",
    "    @staticmethod\n",
    "    def oracle_score(data_tuple):\n",
    "        dset, loader, evaluator = data_tuple\n",
    "        quesid2ans = {}\n",
    "        for i, (ques_id, feats, boxes, sent, target, img_id) in enumerate(loader):\n",
    "            if args.multiclass == True:\n",
    "                label = target\n",
    "            else:\n",
    "                _, label = target.max(1)\n",
    "            for qid, l in zip(ques_id, label.cpu().numpy()):\n",
    "                ans = dset.label2ans[l]\n",
    "                quesid2ans[qid.item()] = ans\n",
    "        return evaluator.evaluate(quesid2ans)\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(),\n",
    "                   os.path.join(self.output, \"%s.pth\" % name))\n",
    "\n",
    "    def load(self, path):\n",
    "        print(\"Load model from %s\" % path)\n",
    "        state_dict = torch.load(\"%s.pth\" % path)\n",
    "        self.model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ccf3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyleft 2019 project LXRT.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# from param import args\n",
    "from utils import load_obj_tsv\n",
    "\n",
    "# Load part of the dataset for fast checking.\n",
    "# Notice that here is the number of images instead of the number of data,\n",
    "# which means all related data to the images would be used.\n",
    "TINY_IMG_NUM = 512\n",
    "FAST_IMG_NUM = 5000\n",
    "\n",
    "# The path to data and image features.\n",
    "VQA_DATA_ROOT = '../../data/vqa/'\n",
    "MSCOCO_IMGFEAT_ROOT = '../../data/mscoco_imgfeat/'\n",
    "SPLIT2NAME = {\n",
    "    'train': 'train2014',\n",
    "    'valid': 'val2014',\n",
    "    'minival': 'val2014',\n",
    "    'nominival': 'val2014',\n",
    "    'test': 'test2015',\n",
    "}\n",
    "\n",
    "\n",
    "class VQADataset:\n",
    "    \"\"\"\n",
    "    A VQA data example in json file:\n",
    "        {\n",
    "            \"answer_type\": \"other\",\n",
    "            \"img_id\": \"COCO_train2014_000000458752\",\n",
    "            \"label\": {\n",
    "                \"net\": 1\n",
    "            },\n",
    "            \"question_id\": 458752000,\n",
    "            \"question_type\": \"what is this\",\n",
    "            \"sent\": \"What is this photo taken looking through?\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, splits: str, subset: str, sampling_ids: str):\n",
    "        self.name = splits\n",
    "        self.splits = splits.split(',')\n",
    "        self.subset = subset # training on subsets: multiclass \n",
    "        self.sampling_ids = sampling_ids\n",
    "\n",
    "        loaded_data = []\n",
    "        for split in self.splits:\n",
    "            loaded_data.extend(json.load(open(\"../../data/vqa/%s.json\" % split)))\n",
    "\n",
    "        # exclude examples with no labels\n",
    "        self.data = []\n",
    "        # Loading datasets, if no subset is specified, train on full dataset\n",
    "\n",
    "        print(\"Loading full multilabel classification dataset\")\n",
    "        if self.sampling_ids != None:\n",
    "            with open(self.sampling_ids, 'rb') as f:\n",
    "                self.sampled_ids = pickle.load(f)\n",
    "            print(\"ids length: \", len(self.sampled_ids))\n",
    "\n",
    "        # for datum in loaded_data:\n",
    "        #     # if 'label' in datum:\n",
    "        #     #     if len(datum['label']) > 0:\n",
    "        #     self.data.append(datum)\n",
    "\n",
    "        # Loading datasets\n",
    "        loaded_data = []\n",
    "        for split in self.splits:\n",
    "            loaded_data.extend(json.load(open(\"../../data/vqa/%s.json\" % split)))\n",
    "        #print(\"Load %d data from split(s) %s.\" % (len(self.data), self.name))\n",
    "        self.data = []\n",
    "\n",
    "        for datum in loaded_data:\n",
    "            if 'minival' in self.splits:\n",
    "                    self.data.append(datum)\n",
    "            else:\n",
    "                if self.sampling_ids != None:                            \n",
    "                    if datum['question_id'] in self.sampled_ids:\n",
    "                        self.data.append(datum)\n",
    "                else:\n",
    "                    self.data.append(datum)   \n",
    "\n",
    "        print(\"Load %d data from split(s) %s.\" % (len(self.data), self.name))\n",
    "\n",
    "        # Convert list to dict (for evaluation)\n",
    "        self.id2datum = {\n",
    "            datum['question_id']: datum\n",
    "            for datum in self.data\n",
    "        }\n",
    "\n",
    "        # Answers\n",
    "        self.ans2label = json.load(open(\"../../data/vqa/trainval_ans2label.json\"))\n",
    "        self.label2ans = json.load(open(\"../../data/vqa/trainval_label2ans.json\"))\n",
    "        assert len(self.ans2label) == len(self.label2ans)\n",
    "\n",
    "    @property\n",
    "    def num_answers(self):\n",
    "        return len(self.ans2label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "An example in obj36 tsv:\n",
    "FIELDNAMES = [\"img_id\", \"img_h\", \"img_w\", \"objects_id\", \"objects_conf\",\n",
    "              \"attrs_id\", \"attrs_conf\", \"num_boxes\", \"boxes\", \"features\"]\n",
    "FIELDNAMES would be keys in the dict returned by load_obj_tsv.\n",
    "\"\"\"\n",
    "class VQATorchDataset(Dataset):\n",
    "    def __init__(self, dataset: VQADataset):\n",
    "        super().__init__()\n",
    "        self.raw_dataset = dataset\n",
    "        if args.tiny:\n",
    "            topk = TINY_IMG_NUM\n",
    "        elif args.fast:\n",
    "            topk = FAST_IMG_NUM\n",
    "        else:\n",
    "            topk = None\n",
    "\n",
    "        # Loading detection features to img_data\n",
    "        img_data = []\n",
    "        for split in dataset.splits:\n",
    "            # Minival is 5K images in MS COCO, which is used in evaluating VQA/LXMERT-pre-training.\n",
    "            # It is saved as the top 5K features in val2014_***.tsv\n",
    "            load_topk = 5000 if (split == 'minival' and topk is None) else topk\n",
    "            img_data.extend(load_obj_tsv(\n",
    "                os.path.join(MSCOCO_IMGFEAT_ROOT, '%s_obj36.tsv' % (SPLIT2NAME[split])),\n",
    "                topk=load_topk))\n",
    "\n",
    "        # Convert img list to dict\n",
    "        self.imgid2img = {}\n",
    "        for img_datum in img_data:\n",
    "            self.imgid2img[img_datum['img_id']] = img_datum\n",
    "\n",
    "        # Only kept the data with loaded image features\n",
    "        self.data = []\n",
    "        for datum in self.raw_dataset.data:\n",
    "            if datum['img_id'] in self.imgid2img:\n",
    "                self.data.append(datum)\n",
    "        print(\"Use %d data in torch dataset\" % (len(self.data)))\n",
    "        print()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        datum = self.data[item]\n",
    "\n",
    "        # while datum['question_id'] in self.exclude_ids:\n",
    "        #     datum = self.data[item+1]\n",
    "\n",
    "\n",
    "        img_id = datum['img_id']\n",
    "        ques_id = datum['question_id']\n",
    "        ques = datum['sent']\n",
    "\n",
    "        # Get image info\n",
    "        img_info = self.imgid2img[img_id]\n",
    "        obj_num = img_info['num_boxes']\n",
    "        feats = img_info['features'].copy()\n",
    "        boxes = img_info['boxes'].copy()\n",
    "        assert obj_num == len(boxes) == len(feats)\n",
    "\n",
    "        # Normalize the boxes (to 0 ~ 1)\n",
    "        img_h, img_w = img_info['img_h'], img_info['img_w']\n",
    "        boxes = boxes.copy()\n",
    "        boxes[:, (0, 2)] /= img_w\n",
    "        boxes[:, (1, 3)] /= img_h\n",
    "        np.testing.assert_array_less(boxes, 1+1e-5)\n",
    "        np.testing.assert_array_less(-boxes, 0+1e-5)\n",
    "\n",
    "        # # Provide label (target)\n",
    "        # if 'label' in datum:\n",
    "        #     label = datum['label']\n",
    "        #     target = torch.zeros(self.raw_dataset.num_answers)\n",
    "        #     for ans, score in label.items():\n",
    "        #         target[self.raw_dataset.ans2label[ans]] = score\n",
    "        #     return ques_id, feats, boxes, ques, target\n",
    "        # else:\n",
    "        #     return ques_id, feats, boxes, ques\n",
    "        # Provide label (target)\n",
    "        if 'label' in datum:\n",
    "            label = datum['label']\n",
    "            target = torch.zeros(self.raw_dataset.num_answers)\n",
    "            if args.multiclass == True:\n",
    "                assert len(label) == 1 # ensure there is only one gold label\n",
    "                for ans, score in label.items():\n",
    "                    #if ans in self.raw_dataset.filtered: # double check the if answer is in filtered category\n",
    "                    target[self.raw_dataset.ans2label[ans]] = 1.0\n",
    "                target = torch.squeeze(target.nonzero())\n",
    "                target = target.long()\n",
    "            else:\n",
    "                for ans, score in label.items():\n",
    "                    target[self.raw_dataset.ans2label[ans]] = score\n",
    "            return ques_id, feats, boxes, ques, target, img_id\n",
    "        else:\n",
    "            return ques_id, feats, boxes, ques\n",
    "\n",
    "\n",
    "class VQAEvaluator:\n",
    "    def __init__(self, dataset: VQADataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def evaluate(self, quesid2ans: dict):\n",
    "        score = 0.\n",
    "        for quesid, ans in quesid2ans.items():\n",
    "            datum = self.dataset.id2datum[quesid]\n",
    "            label = datum['label']\n",
    "            if ans in label:\n",
    "                score += label[ans]\n",
    "        return score / len(quesid2ans)\n",
    "\n",
    "    def dump_result(self, quesid2ans: dict, path):\n",
    "        \"\"\"\n",
    "        Dump results to a json file, which could be submitted to the VQA online evaluation.\n",
    "        VQA json file submission requirement:\n",
    "            results = [result]\n",
    "            result = {\n",
    "                \"question_id\": int,\n",
    "                \"answer\": str\n",
    "            }\n",
    "\n",
    "        :param quesid2ans: dict of quesid --> ans\n",
    "        :param path: The desired path of saved file.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path, 'w') as f:\n",
    "            result = []\n",
    "            for ques_id, ans in quesid2ans.items():\n",
    "                datum = self.dataset.id2datum[ques_id]\n",
    "                answer_type = datum['answer_type']\n",
    "                img_id = datum['img_id']\n",
    "                label = datum['label']\n",
    "                question_type = datum['question_type']\n",
    "                question = datum['sent']\n",
    "                score = 0.\n",
    "                if ans in label:\n",
    "                    score += label[ans]\n",
    "                \n",
    "\n",
    "                result.append({\n",
    "                    'question_id': ques_id,\n",
    "                    'answer': ans,\n",
    "                    'answer_type': answer_type,\n",
    "                    'img_id': img_id,\n",
    "                    'label': label,\n",
    "                    'question_type': question_type,\n",
    "                    'question': question,\n",
    "                    'score': score\n",
    "                })\n",
    "            json.dump(result, f, indent=4, sort_keys=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2fec497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(occurences):\n",
    "    if occurences == 0:\n",
    "        return 0\n",
    "    elif occurences == 1:\n",
    "        return 0.3\n",
    "    elif occurences == 2:\n",
    "        return 0.6\n",
    "    elif occurences == 3:\n",
    "        return 0.9\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "372526c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full multilabel classification dataset\n",
      "Load 443757 data from split(s) train.\n",
      "Start to load Faster-RCNN detected objects from ../../data/mscoco_imgfeat/train2014_obj36.tsv\n",
      "Loaded 82783 images in file ../../data/mscoco_imgfeat/train2014_obj36.tsv in 305 seconds.\n",
      "Use 443648 data in torch dataset\n",
      "\n",
      "LXRT encoder with 1 l_layers, 1 x_layers, and 1 r_layers.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 101: invalid device ordinal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vqa \u001b[38;5;241m=\u001b[39m \u001b[43mVQA\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36mVQA.__init__\u001b[0;34m(self, sampling_ids)\u001b[0m\n\u001b[1;32m     55\u001b[0m     load_lxmert_qa(args\u001b[38;5;241m.\u001b[39mload_lxmert_qa, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     56\u001b[0m                    label2ans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_tuple\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabel2ans)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# GPU options\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmultiGPU:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlxrt_encoder\u001b[38;5;241m.\u001b[39mmulti_gpu()\n",
      "File \u001b[0;32m~/.conda/envs/lxmert/lib/python3.9/site-packages/torch/nn/modules/module.py:689\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/lxmert/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/lxmert/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 579 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/lxmert/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/lxmert/lib/python3.9/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.conda/envs/lxmert/lib/python3.9/site-packages/torch/nn/modules/module.py:689\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/lxmert/lib/python3.9/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 101: invalid device ordinal"
     ]
    }
   ],
   "source": [
    "vqa = VQA()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.load is not None:\n",
    "    vqa.load(args.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vqa.evaluate(\n",
    "    get_data_tuple('minival', args.subset, bs=950,\n",
    "                shuffle=False, drop_last=False)\n",
    ")\n",
    "# dump=os.path.join(args.output, 'minival_predict.json')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Neptune logging\n",
    "#     print(\"SEED: \", args.seed)\n",
    "\n",
    "    # Build Class\n",
    "    vqa = VQA(args.sampling_ids)\n",
    "\n",
    "    # Load VQA model weights\n",
    "    # Note: It is different from loading LXMERT pre-trained weights.\n",
    "    if args.load is not None:\n",
    "        vqa.load(args.load)\n",
    "\n",
    "#     # Test or Train\n",
    "#     if args.test is not None:\n",
    "#         args.fast = args.tiny = False       # Always loading all data in test\n",
    "#         if 'test' in args.test:\n",
    "#             vqa.predict(\n",
    "#                 get_data_tuple(args.test, args.subset, bs=950,\n",
    "#                                shuffle=False, drop_last=False),\n",
    "#                 dump=os.path.join(args.output, 'test_predict.json')\n",
    "#             )\n",
    "#         elif 'val' in args.test:    \n",
    "#             # Since part of valididation data are used in pre-training/fine-tuning,\n",
    "#             # only validate on the minival set.\n",
    "#             if args.test == 'gqa_ood_val':\n",
    "#                 print(\"GQA OOD\")\n",
    "#                 result = vqa.evaluate(\n",
    "#                         get_gqa_tuple('train,valid,testdev', subset=args.test, bs=512,\n",
    "#                                 shuffle=False, drop_last=False), \n",
    "#                     train_label2ans=vqa.train_tuple.dataset.label2ans, \n",
    "#                     dump=os.path.join(args.output, 'minival_predict.json')\n",
    "#                 )\n",
    "\n",
    "#                 print(result)\n",
    "#             else:\n",
    "    result = vqa.evaluate(\n",
    "        get_data_tuple('minival', args.subset, bs=950,\n",
    "                    shuffle=False, drop_last=False),\n",
    "        dump=os.path.join(args.output, 'minival_predict.json')\n",
    "    )\n",
    "    print(result)\n",
    "            \n",
    "#         else:\n",
    "#             assert False, \"No such test option for %s\" % args.test\n",
    "#     else:\n",
    "#         print('Splits in Train data:', vqa.train_tuple.dataset.splits)\n",
    "#         if vqa.valid_tuple is not None:\n",
    "#             print('Splits in Valid data:', vqa.valid_tuple.dataset.splits)\n",
    "#             print(\"Valid Oracle: %0.2f\" % (vqa.oracle_score(vqa.valid_tuple) * 100))\n",
    "#         else:\n",
    "#             print(\"DO NOT USE VALIDATION\")\n",
    "#         vqa.train(vqa.train_tuple, vqa.valid_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df8c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lxmert",
   "language": "python",
   "name": "lxmert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
