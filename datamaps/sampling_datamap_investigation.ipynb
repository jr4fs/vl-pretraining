{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re \n",
    "import seaborn as sns\n",
    "import math \n",
    "import collections\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "import plotly.express as px\n",
    "# from jupyter_dash import JupyterDash\n",
    "# from dash import Dash, dcc, html, Input, Output, no_update\n",
    "import plotly.graph_objects as go\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import base64\n",
    "import itertools\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "sns.set(style='whitegrid', font_scale=1.6, font='Georgia', context='paper')\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datamap_stats(base_path):\n",
    "    '''\n",
    "    Returns datamap stats recorded during training split by epochs\n",
    "\n",
    "            Parameters:\n",
    "                    base_path (str): Path to model metadata\n",
    "\n",
    "            Returns:\n",
    "                    df (DataFrame): Pandas dataframe with datamap stats: \n",
    "                                    confidence, variability and correctness\n",
    "    '''\n",
    "    # load logged stats from training \n",
    "    with open(base_path + 'datamaps_stats.json') as fp:\n",
    "        datamap_stats_raw = json.load(fp)\n",
    "\n",
    "    result = collections.defaultdict(list)\n",
    "    for stat in datamap_stats_raw:\n",
    "        result[stat['Epoch']].append(stat)\n",
    "    datamap_stats = list(result.values())\n",
    "    #datamap_stats = list(datamap_stats.values())\n",
    "\n",
    "    return datamap_stats\n",
    "\n",
    "def calc_confidence(df_probabilities):\n",
    "    '''Calculates confidence by taking the mean for each instance probability at ground truth over all epochs'''\n",
    "    x = np.array(df_probabilities['Probabilities'].tolist())\n",
    "    confidence = np.mean(x.astype(float), axis=1)\n",
    "    return confidence\n",
    "\n",
    "def calc_variance(df_probabilities):\n",
    "    '''Calculates variance by taking the standard deviation for each instance probability at ground truth over all epochs'''\n",
    "    x = np.array(df_probabilities['Probabilities'].tolist())\n",
    "    variance = np.std(x.astype(float), axis=1)\n",
    "    return variance\n",
    "\n",
    "def calc_correctness(df_correct_preds):\n",
    "    '''Calculates correctness by measuring the number of times prediction == target'''\n",
    "    x = np.array(df_correct_preds['Corrects'].tolist())\n",
    "    boolean_array = (x=='True')\n",
    "    sums = np.sum(boolean_array, axis=1)\n",
    "    correctness = sums/len(df_correct_preds.loc[0]['Corrects'])\n",
    "    return correctness\n",
    "\n",
    "def remove_duplicates(id_probs, probs_corrects, col_name):\n",
    "    predictions = pd.DataFrame(list(zip(id_probs.keys(), probs_corrects)),\n",
    "    columns =['Question ID', col_name])\n",
    "    predictions_duplicate = predictions[col_name].values\n",
    "    predictions_remove_duplications = []\n",
    "    for pred in predictions_duplicate:\n",
    "        update = list(set(pred))\n",
    "        if col_name != 'Predictions':\n",
    "            if len(update) != 1:\n",
    "                print(col_name)\n",
    "                print(update)\n",
    "                print(\"INVALID\")\n",
    "        predictions_remove_duplications.append(list(set(pred)))\n",
    "    predictions[col_name] = predictions_remove_duplications\n",
    "    return predictions\n",
    "\n",
    "def calculate_datamap_metrics(datamap_stats, correctness_check=False):\n",
    "    '''\n",
    "    Returns dataframe with confidence, correctness and variability for datamap plotting\n",
    "\n",
    "            Parameters:\n",
    "                    datamap_stats (list): List of instance metadata for datamap \n",
    "                                          generation bucketed by epoch\n",
    "\n",
    "            Returns:\n",
    "                    df (DataFrame): Pandas dataframe with datamap metrics and question ids\n",
    "    '''\n",
    "    coco_train = COCO('coco/annotations/instances_train2014.json')\n",
    "    coco_val = COCO('coco/annotations/instances_val2014.json')\n",
    "    ids_probs = {}\n",
    "    for example in datamap_stats[0]:\n",
    "        ids_probs[example['Question ID']]=([],[],[],[],[],[]) # probabilities, corrects, question, target, prediction, image_id\n",
    "\n",
    "    for epoch_instances in datamap_stats:\n",
    "        for example in epoch_instances:\n",
    "            if example['Target'] == 'geese' and example['Prediction'] == 'goose': # special case in animal split\n",
    "                correct = True\n",
    "            elif example['Prediction'][-1] == 's' and example['Prediction'][-1] == example['Target']: # account for plurals in animal split\n",
    "                correct = True\n",
    "            elif example['Target'] == example['Prediction']:\n",
    "                correct = True\n",
    "            else:\n",
    "                correct = False\n",
    "            ids_probs[example['Question ID']][0].append(example['GT Probability'])\n",
    "            ids_probs[example['Question ID']][1].append(correct)\n",
    "            ids_probs[example['Question ID']][2].append(example['Question'])\n",
    "            ids_probs[example['Question ID']][3].append(example['Target'])\n",
    "            ids_probs[example['Question ID']][4].append(example['Prediction'])\n",
    "            ids_probs[example['Question ID']][5].append(example['Image ID'])\n",
    "\n",
    "    probs_corrects = np.array(list(ids_probs.values()))\n",
    "    probabilities = pd.DataFrame(list(zip(ids_probs.keys(), probs_corrects[:, 0, :])),\n",
    "                columns =['Question ID', 'Probabilities'])\n",
    "\n",
    "    corrects = pd.DataFrame(list(zip(ids_probs.keys(), probs_corrects[:, 1, :])),\n",
    "                columns =['Question ID', 'Corrects'])\n",
    "\n",
    "    predictions = remove_duplicates(ids_probs, probs_corrects[:, 4, :], 'Predictions')\n",
    "    questions = remove_duplicates(ids_probs, np.squeeze(probs_corrects[:, 2, :]), 'Question')\n",
    "    targets = remove_duplicates(ids_probs, np.squeeze(probs_corrects[:, 3, :]), 'Target')\n",
    "    image_ids = remove_duplicates(ids_probs, np.squeeze(probs_corrects[:, 5, :]), 'Image ID')\n",
    "    ids_list = image_ids['Image ID']\n",
    "    img_urls = []\n",
    "    for img in ids_list:\n",
    "        assert len(img) == 1\n",
    "        id_str = str(img[0])\n",
    "        if 'val' in id_str:\n",
    "            loaded_img = coco_val.loadImgs([int(id_str[-6:])])\n",
    "        else:\n",
    "            loaded_img = coco_train.loadImgs([int(id_str[-6:])])\n",
    "        \n",
    "        assert len(loaded_img) == 1\n",
    "        img_urls.append(loaded_img[0]['coco_url'])\n",
    "\n",
    "    confidence_score = calc_confidence(probabilities)\n",
    "    variance_score = calc_variance(probabilities)\n",
    "    correctness_score = calc_correctness(corrects)\n",
    "\n",
    "    d= {\"confidence\": np.squeeze(confidence_score), \"variability\": np.squeeze(variance_score), \"correctness\":correctness_score, \"question_id\":probabilities['Question ID'],\n",
    "        \"Predictions\": predictions['Predictions'], \"Question\":questions['Question'], \"Target\":targets['Target'], \"Image ID\":ids_list, \"Image URL\":img_urls}\n",
    "    df= pd.DataFrame(d)\n",
    "\n",
    "    if correctness_check == True:\n",
    "        low_correctness_high_confidence = df.loc[(df['correctness'] < 0.4) & (df['confidence'] > 0.8)]\n",
    "\n",
    "        ids = low_correctness_high_confidence.index\n",
    "\n",
    "        for id in ids:  \n",
    "            x = probabilities.loc[id]\n",
    "            y = corrects.loc[id]\n",
    "\n",
    "            d_exp = {'probabilities':x, 'correctness': y}\n",
    "            df_exp = pd.DataFrame(data=d)\n",
    "            print(df_exp)\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_trainval_acc(base_path):\n",
    "    '''\n",
    "    Plots train/val accuracy scores \n",
    "\n",
    "            Parameters:\n",
    "                    base_path (str): Path to model metadata\n",
    "\n",
    "    '''\n",
    "    with open(base_path + 'log.log') as fp:\n",
    "        acc = fp.readlines()\n",
    "    train_scores = []\n",
    "    valid_scores = []\n",
    "    for i in acc:\n",
    "        if 'Train' in i:\n",
    "            train_scores.append(float(i[-7:].strip()))\n",
    "        elif 'Valid' in i:\n",
    "            valid_scores.append(float(i[-7:].strip()))\n",
    "\n",
    "    xs_valid = [i for i in range(len(valid_scores))]\n",
    "    xs_train = [i for i in range(len(train_scores))]\n",
    "    plt.plot(xs_valid, valid_scores, label=\"Validation\")\n",
    "    #plt.title(\"Validation\")\n",
    "    plt.savefig(base_path+'/training.png')\n",
    "\n",
    "    plt.plot(xs_train, train_scores, label=\"Training\")\n",
    "    #plt.title(\"Training\")\n",
    "    plt.savefig(base_path+'/validation.png')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Correct Preds\")\n",
    "\n",
    "def calc_datamap_metrics(base_path):\n",
    "    datamap_stats = load_datamap_stats(base_path)\n",
    "    df = calculate_datamap_metrics(datamap_stats)\n",
    "    df.to_pickle(base_path+'datamap_metrics.pkl')\n",
    "    \n",
    "def scatter_it(dataframe, hue_metric ='correct.', title='', model='LXMERT', show_hist=False):\n",
    "    # Subsample data to plot, so the plot is not too busy.\n",
    "    dataframe = dataframe.sample(n=25000 if dataframe.shape[0] > 25000 else len(dataframe))\n",
    "    \n",
    "    # Normalize correctness to a value between 0 and 1.\n",
    "    #dataframe = dataframe.assign(corr_frac = lambda d: d.correctness / d.correctness.max())\n",
    "    #dataframe['correct.'] = [f\"{x:.1f}\" for x in dataframe['corr_frac']]\n",
    "    dataframe['correct.'] = dataframe['correctness']\n",
    "    \n",
    "    main_metric = 'variability'\n",
    "    other_metric = 'confidence'\n",
    "    \n",
    "    hue = hue_metric\n",
    "    num_hues = len(dataframe[hue].unique().tolist())\n",
    "    style = hue_metric if num_hues < 8 else None\n",
    "\n",
    "    if not show_hist:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(8, 4))\n",
    "        ax0 = axs\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(16, 10), )\n",
    "        gs = fig.add_gridspec(2, 3, height_ratios=[5, 1])\n",
    "    \n",
    "        ax0 = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    \n",
    "    ### Make the scatterplot.\n",
    "    \n",
    "    # Choose a palette.\n",
    "    pal = sns.diverging_palette(260, 15, n=num_hues, sep=10, center=\"dark\")\n",
    "    pal.reverse()\n",
    "\n",
    "    plot = sns.scatterplot(x=main_metric,\n",
    "                           y=other_metric,\n",
    "                           ax=ax0,\n",
    "                           data=dataframe,\n",
    "                           hue=hue,\n",
    "                           palette=pal,\n",
    "                           style=style,\n",
    "                           s=30)\n",
    "    \n",
    "    # Annotate Regions.\n",
    "    bb = lambda c: dict(boxstyle=\"round,pad=0.3\", ec=c, lw=2, fc=\"white\")\n",
    "    an1 = ax0.annotate(\"ambiguous\", xy=(0.9, 0.5), xycoords=\"axes fraction\", fontsize=15, color='black',\n",
    "                  va=\"center\", ha=\"center\", rotation=350, bbox=bb('black'))\n",
    "    an2 = ax0.annotate(\"easy-to-learn\", xy=(0.27, 0.85), xycoords=\"axes fraction\", fontsize=15, color='black',\n",
    "                  va=\"center\", ha=\"center\", bbox=bb('r'))\n",
    "    an3 = ax0.annotate(\"hard-to-learn\", xy=(0.35, 0.25), xycoords=\"axes fraction\", fontsize=15, color='black',\n",
    "                  va=\"center\", ha=\"center\", bbox=bb('b'))\n",
    "    \n",
    "    if not show_hist:\n",
    "        plot.legend(ncol=1, bbox_to_anchor=(1.01, 0), loc='center left', fancybox=True, shadow=True)\n",
    "    else:\n",
    "        plot.legend(fancybox=True, shadow=False,  ncol=1)\n",
    "    plot.set_xlabel('variability')\n",
    "    plot.set_ylabel('confidence')\n",
    "\n",
    "\n",
    "    # fig = px.scatter(dataframe, x=main_metric,\n",
    "    #                        y=other_metric, \n",
    "    #                        color=hue,\n",
    "    #                        symbol=style,\n",
    "    #                        size_max=30)\n",
    "    # fig.show()\n",
    "\n",
    "    \n",
    "    if show_hist:\n",
    "        plot.set_title(f\"{model}-{title} Data Map\", fontsize=17)\n",
    "        \n",
    "        # Make the histograms.\n",
    "        ax1 = fig.add_subplot(gs[1, 0])\n",
    "        ax2 = fig.add_subplot(gs[1, 1])\n",
    "        ax3 = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "        plott0 = dataframe.hist(column=['confidence'], ax=ax1, color='#622a87')\n",
    "        plott0[0].set_title('')\n",
    "        plott0[0].set_xlabel('confidence')\n",
    "        plott0[0].set_ylabel('density')\n",
    "\n",
    "        plott1 = dataframe.hist(column=['variability'], ax=ax2, color='teal', range=[0,1])\n",
    "        plott1[0].set_title('')\n",
    "        plott1[0].set_xlabel('variability')\n",
    "\n",
    "        #plot2 = sns.countplot(x=\"correct.\", data=dataframe, color='#86bf91', ax=ax3)\n",
    "        plot2 = dataframe.hist(column=['correct.'], ax=ax3, color='#86bf91')\n",
    "        ax3.xaxis.grid(True) # Show the vertical gridlines\n",
    "\n",
    "        plot2[0].set_title('')\n",
    "        plot2[0].set_xlabel('correctness')\n",
    "        plot2[0].set_ylabel('')\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path ='../snap/vqa/LXR111/animals/other/vqa_lxr111_animals_fromScratch_20epochs_breeds/'\n",
    "# datamap_stats = load_datamap_stats(base_path)\n",
    "# df = calculate_datamap_metrics(datamap_stats)\n",
    "df = pd.read_pickle(base_path+\"datamap_metrics.pkl\")\n",
    "plot_trainval_acc(base_path)\n",
    "scatter_it(df, title=' Trained on VQA-Animals Trained from Scratch for 20 epochs', show_hist=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Sep 28 2021, 16:10:42) \n[GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
